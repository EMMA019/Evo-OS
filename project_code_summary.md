# Project Code Summary

Generated on: 2025-12-07 12:39:56

## File: `Dockerfile`

dockerfile
# ãƒ•ã‚¡ã‚¤ãƒ«å: Dockerfile
# å½¹å‰²: Evo OS Core ãŒã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®å®‰å…¨ãªã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ç’°å¢ƒå®šç¾©
# ãƒ“ãƒ«ãƒ‰ã‚³ãƒžãƒ³ãƒ‰: docker build -t evo-sandbox .

# è»½é‡ã‹ã¤å®‰å®šã—ãŸPythonç’°å¢ƒã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹
FROM python:3.10-slim

# ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# build-essential: Cæ‹¡å¼µã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«å¿…è¦ (numpyãªã©)
# git, curl: ä¸€èˆ¬çš„ãªãƒ„ãƒ¼ãƒ«
# libxml2-dev, libxslt-dev: lxmlãªã©ã®ãƒ‘ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç”¨
# nodejs, npm: React/Frontendã®ãƒ“ãƒ«ãƒ‰ç”¨
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    curl \
    libxml2-dev \
    libxslt-dev \
    nodejs \
    npm \
    && rm -rf /var/lib/apt/lists/*

# ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®è¨­å®šï¼ˆagent_core.pyã®ãƒžã‚¦ãƒ³ãƒˆå…ˆï¼‰
WORKDIR /workspace

# ã‚ˆãä½¿ã‚ã‚Œã‚‹Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# ã“ã‚Œã«ã‚ˆã‚Šã€AIãŒç”Ÿæˆã—ãŸã‚³ãƒ¼ãƒ‰ã® ModuleNotFoundError ã‚’é˜²ãŽã€å®Ÿè¡Œé€Ÿåº¦ã‚’ä¸Šã’ã‚‹
# Qiskitãªã©ã®é‡ã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚‚å«ã‚ã‚‹ã“ã¨ã§ã€Œåœ°ç„ç´šã€ã‚¿ã‚¹ã‚¯ã«ã‚‚å¯¾å¿œ
RUN pip install --no-cache-dir \
    numpy \
    pandas \
    flask \
    requests \
    beautifulsoup4 \
    lxml \
    matplotlib \
    pytest \
    scipy \
    scikit-learn \
    qiskit \
    fastapi \
    uvicorn \
    websockets

# ã‚³ãƒ³ãƒ†ãƒŠãŒå‹æ‰‹ã«çµ‚äº†ã—ãªã„ã‚ˆã†ã«ã™ã‚‹ï¼ˆagent_core.pyãŒ exec ã§å…¥ã‚‹ãŸã‚ï¼‰
CMD ["sleep", "infinity"]


## File: `agent_core.py`

py
import os
import sys
import time
import json
import logging
import subprocess
import atexit
import uuid
import shutil
import contextvars
import re
from typing import Dict, List, Optional

# è¨­å®šã¨ã‚µãƒ¼ãƒ“ã‚¹ç¾¤ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.config import config
from src.services.budget_service import BudgetGuard
from src.services.workspace_manager import WorkspaceManager
from src.services.architect_service import ArchitectService
from src.services.kit_manager import KitManager
from src.services.kit_gen_service import KitGenService
from src.services.search_service import SearchService
from src.services.qa_service import QualityAssuranceService
from src.services.verifier_service import VerifierService
from src.services.healer_service import HealerService
from src.services.structure_service import StructureService
from src.services.data_recorder import DataRecorder

# ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¯ãƒ©ã‚¹ã®å®šç¾©ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚ã“ã“ã«é…ç½®ã€‚åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«åˆ†é›¢ã‚’æŽ¨å¥¨ï¼‰
class BaseRuntime:
    def start(self): pass
    def stop(self): pass
    def install_requirements(self): pass
    def test_run(self, entry_point): return True, "No runtime"
class DockerRuntime(BaseRuntime):
    def __init__(self):
        self.container = f"{config.CONTAINER_PREFIX}-{uuid.uuid4().hex[:8]}"
        self.workdir = os.path.abspath(config.OUTPUT_DIR)
        self._started = False
        self._available = bool(shutil.which("docker"))
        if self._available:
            try: subprocess.run(["docker", "info"], capture_output=True, check=True)
            except: self._available = False
    def start(self):
        if not self._available or self._started: return
        self._cleanup()
        try:
            env_args = ["-e", f"GOOGLE_API_KEY={config.LLM_API_KEY}"]
            subprocess.run(
                ["docker", "run", "-d", "--rm", "--name", self.container, "--network", "host", "-v", f"{self.workdir}:/workspace"] + env_args + [config.DOCKER_IMAGE, "sleep", "infinity"], 
                check=True, capture_output=True
            )
            self._started = True; atexit.register(self.stop)
            logger.info("ðŸ³ Docker Runtime Started.")
        except Exception as e: 
            logger.warning(f"âš ï¸ Docker failed: {e}. Falling back to Local.")
            self._available = False
    def stop(self):
        if self._started: 
            subprocess.run(["docker", "rm", "-f", self.container], capture_output=True)
            self._started = False
            logger.info("ðŸ³ Docker Runtime Stopped.")
    def _cleanup(self): self.stop()
    def install_requirements(self):
        if not self._started: return
        if os.path.exists(os.path.join(self.workdir, "requirements.txt")):
            logger.info("ðŸ“¦ Docker: Installing requirements...")
            subprocess.run(["docker", "exec", "-w", "/workspace", self.container, "pip", "install", "-r", "requirements.txt"], capture_output=True, timeout=120)
    def test_run(self, entry_point="app.py"):
        if not self._started: return False, "Docker not started"
        try:
            cmd = ["docker", "exec", "-w", "/workspace", self.container, "python", entry_point]
            proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            try: outs, errs = proc.communicate(timeout=10) 
            except subprocess.TimeoutExpired: proc.kill(); return True, "Running"
            if proc.returncode != 0: return False, f"Error:\n{errs}"
            return True, "Success"
        except Exception as e: return False, str(e)
class LocalRuntime(BaseRuntime):
    def __init__(self):
        self.workdir = os.path.abspath(config.OUTPUT_DIR)
        self.venv_dir = os.path.join(self.workdir, ".venv")
        is_win = os.name == 'nt'
        self.py_exe = os.path.join(self.venv_dir, "Scripts" if is_win else "bin", "python.exe" if is_win else "python")
    def start(self):
        if not os.path.exists(self.py_exe):
            logger.info("ðŸ Creating Local venv...")
            subprocess.run([sys.executable, "-m", "venv", self.venv_dir], check=True)
        logger.info("ðŸ Local Runtime Ready.")
    def install_requirements(self):
        req = os.path.join(self.workdir, "requirements.txt")
        if os.path.exists(req):
            logger.info("ðŸ“¦ Local: Installing requirements...")
            try: subprocess.run([self.py_exe, "-m", "pip", "install", "-r", req], cwd=self.workdir, capture_output=True, check=True, timeout=120)
            except: pass
    def test_run(self, entry_point="app.py"):
        if not os.path.exists(os.path.join(self.workdir, entry_point)): return False, "File not found"
        logger.info(f"ðŸ§ª Local Testing: {entry_point}...")
        try:
            env = os.environ.copy()
            env["GOOGLE_API_KEY"] = config.LLM_API_KEY
            proc = subprocess.Popen([self.py_exe, entry_point], cwd=self.workdir, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            try: outs, errs = proc.communicate(timeout=10)
            except subprocess.TimeoutExpired: proc.kill(); return True, "Running"
            if proc.returncode != 0: return False, f"Error:\n{errs}\n{outs}"
            return True, "Success"
        except Exception as e: return False, str(e)


# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger("EvoCore")

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: ãƒ‘ã‚¹ã®å®‰å…¨æ€§ç¢ºä¿ ---
def safe_path_join(base, *paths):
    final_path = os.path.abspath(os.path.join(base, *paths))
    if not final_path.startswith(os.path.abspath(base)): raise ValueError("Path traversal attempt")
    return final_path

# --- AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š ---
try:
    import google.generativeai as genai
    if config.LLM_API_KEY: genai.configure(api_key=config.LLM_API_KEY)
except: pass

class ResilientClient:
    """LLMå‘¼ã³å‡ºã—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: configã®MAX_RETRIESã«ä¾å­˜"""
    def __init__(self, model, budget_guard):
        self.model = genai.GenerativeModel(model)
        self.budget = budget_guard
        self.name = model

    def generate(self, prompt, sys_prompt="") -> str:
        full_prompt = f"{sys_prompt}\n\n{prompt}"
        
        # configã®MAX_RETRIESã‚’ä½¿ç”¨ (ç¾åœ¨ã¯1)
        for i in range(config.MAX_RETRIES):
            try:
                res = self.model.generate_content(full_prompt)
                text = res.text.strip()
                self.budget.check_and_record(self.name, len(full_prompt), len(text))
                return text
            except Exception as e:
                if "Budget" in str(e): raise e
                logger.warning(f"âš ï¸ GenAI Error ({i+1}/{config.MAX_RETRIES}): {e}")
                time.sleep(1)
        raise RuntimeError("LLM Error: Failed after all retries.")


class Orchestrator:
    """
    ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¸ä»¤å¡”ï¼ˆGod Objectã®åˆ†é›¢å®Œäº†ï¼‰ã€‚
    å„å°‚é–€ã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã³å‡ºã™ã“ã¨ã«å¾¹ã™ã‚‹ã€‚
    """
    def __init__(self):
        self.logs = []
        self.budget = BudgetGuard(config.MAX_BUDGET_PER_RUN)
        
        # 1. Workspace & Git (é›‘å‹™ä¿‚)
        self.ws = WorkspaceManager()
        
        # 2. AI Clients (å…¨ãƒ¢ãƒ‡ãƒ«ã‚’Standard Flashã«çµ±ä¸€æ¸ˆã¿)
        client_fast = ResilientClient(config.LLM_MODEL_FAST, self.budget)
        client_smart = ResilientClient(config.LLM_MODEL_SMART, self.budget)
        client_healer = ResilientClient(config.LLM_MODEL_HEALER, self.budget)
        
        # 3. Services (ä¾å­˜é–¢ä¿‚ã®æ³¨å…¥)
        self.kit_mgr = KitManager(client_fast)
        # Architectã¯KitManagerã«ä¾å­˜ã™ã‚‹
        self.architect = ArchitectService(client_smart, self.kit_mgr)
        
        self.verifier = VerifierService(None) # Runtimeã¯å¾Œã§æ³¨å…¥
        self.healer = HealerService(client_fast, client_healer)
        self.qa = QualityAssuranceService(client_smart)
        self.structure = StructureService()
        self.search = SearchService(client_fast)
        self.kit_gen = KitGenService(client_smart)
        self.recorder = DataRecorder()

        # 4. Runtime
        self.docker = DockerRuntime()
        self.runtime = self.docker if self.docker._available else LocalRuntime()
        self.runtime.start()
        self.verifier.runtime = self.runtime # Verifierã«Runtimeã‚’æ³¨å…¥

        # å®Ÿè¡Œä¸­ã®Kitæƒ…å ±ã‚’ä¿æŒ
        self.current_kit = None 

    def log(self, msg):
        logger.info(msg)
        self.logs.append(msg)

    def cleanup(self):
        self.runtime.stop()

    def run(self, prompt: str) -> Dict:
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ•ãƒ­ãƒ¼: ç›´åˆ—çš„ã§èª­ã¿ã‚„ã™ã„æ§‹é€ """
        self.log(f"ðŸš€ Evo Started: {prompt[:30]}...")
        
        try:
            # A. ç‰¹æ®Šãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
            if any(k in prompt for k in ["ã‚­ãƒƒãƒˆã‚’ä½œã£ã¦", "Kitã‚’ä½œã£ã¦", "Create Kit"]):
                return self._mode_kit_gen(prompt)
            if any(k in prompt.lower() for k in ["èª¿ã¹ã¦", "search", "research"]):
                return self._mode_research(prompt)

            # B. æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º: è¨ˆç”»ä½œæˆã¨ã‚­ãƒƒãƒˆé¸æŠžã‚’ä¸€åº¦ã«è¡Œã†
            plan, kit = self.architect.create_plan(prompt)
            
            self.current_kit = kit
            if kit: self.log(f"ðŸ§© Kit Confirmed: {kit['name']}")
            
            # C. å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º (Phase Execution)
            for step in plan:
                self.log(f"ðŸ—ï¸ Phase {step['phase']}: {step['description']}")
                self._execute_phase(step, prompt, kit)
                self.ws.commit(f"Phase {step['phase']} Done")

            # D. æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º (Runtime Check)
            self._runtime_check(kit)

            # E. ç›£æŸ»ãƒ•ã‚§ãƒ¼ã‚º (QA)
            self._final_audit()

            # F. ä¿å­˜
            self.recorder.save_success(prompt, kit['name'] if kit else None, self.ws.project_files)
            
            return {
                "success": True, 
                "files": self.ws.project_files, 
                "logs": self.logs,
                "kit_used": kit['name'] if kit else None
            }

        except Exception as e:
            self.log(f"ðŸ’¥ Fatal Error: {e}")
            return {"success": False, "error": str(e), "logs": self.logs}
        finally:
            self.cleanup()

    # --- Sub Routines (ãƒ­ã‚¸ãƒƒã‚¯ã‚’åˆ†é›¢) ---

    def _execute_phase(self, phase, original_prompt, kit):
        """ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã¨é™çš„ãƒ’ãƒ¼ãƒªãƒ³ã‚°ï¼ˆ1å›žï¼‰"""
        target_files = phase.get('files', [])
        if not target_files: return
        
        # æ§‹é€ è§£æž
        struct_map = self.structure.analyze_project(self.ws.project_files)
        
        for target_file in target_files: # â˜…ã“ã“ãŒå®Ÿè¡Œã®ãƒˆãƒªã‚¬ãƒ¼ã«ãªã‚‹
            self.log(f"ðŸ“ Coding: {target_file}")
            
            # 1. ç”Ÿæˆ (Generation)
            kit_rules = ""
            if kit: kit_rules += f"\nKit Rules: {kit.get('name')}"

            sys_prompt = f"""
            Role: Expert Developer. Task: Write code for '{target_file}'.
            Map:\n{struct_map}
            {kit_rules}
            Important: Implement FULL code. Output ONLY the code.
            """
            
            # â˜… ä¿®æ­£æ¸ˆã¿: LLMã‹ã‚‰ raw_response ã‚’å–å¾—
            raw_response = self.architect.client.generate(f"Goal: {original_prompt}\nFile: {target_file}", sys_prompt)
            
            # 2. ä¿å­˜ (Save)
            # raw_response ã‚’ parse_and_save_files ã«æ¸¡ã™
            new_files = self.ws.parse_and_save_files(raw_response, default_filename=target_file)
            
            # 3. é™çš„ä¿®å¾© (Static Heal) - 1å›žå‹è² 
            for fname in new_files.keys():
                self._static_heal(fname, kit)

    def _static_heal(self, filename, kit):
        """é™çš„ã‚¨ãƒ©ãƒ¼ä¿®å¾©ã®1å›žå‹è² ãƒ­ã‚¸ãƒƒã‚¯"""
        # config.MAX_RETRIES (1å›ž) ã ã‘å›žã‚‹
        for _ in range(config.MAX_RETRIES):
            # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‹ã‚‰æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å–å¾—
            content = self.ws.project_files.get(filename, "")
            
            res = self.verifier.verify(content, filename, self.ws.project_files)
            if res['valid']: break
            
            self.log(f"ðŸ©¹ Static Healing {filename}: {res['errors'][0][:50]}...")
            
            success, fixed, strategy = self.healer.heal(filename, content, res['errors'], self.ws.project_files, kit)
            
            if success and strategy not in ["Loop_Ignored", "Skipped"]:
                self.ws.save_file(filename, fixed)
            else:
                self.log(f"âš ï¸ Static fix skipped for {filename} ({strategy}). Proceeding.")
                break 

    def _runtime_check(self, kit):
        """ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãƒã‚§ãƒƒã‚¯ã¨ãƒ’ãƒ¼ãƒªãƒ³ã‚°ï¼ˆ1å›žå‹è² ï¼‰"""
        entry = next((f for f in ["app.py", "main.py"] if f in self.ws.project_files), None)
        if not entry: return

        self.log(f"ðŸ§ª Runtime Test: {entry}")
        self.runtime.install_requirements()
        
        # 1å›žå‹è² 
        for _ in range(config.MAX_RETRIES):
            ok, log = self.runtime.test_run(entry)
            if ok: 
                self.log("âœ… Runtime OK")
                return
            
            # ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼ãªã‚‰å³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãƒªãƒˆãƒ©ã‚¤
            if "ModuleNotFoundError" in log:
                missing = self._extract_module(log)
                if missing:
                    self.log(f"ðŸ“¦ Installing missing: {missing}")
                    self.ws.add_to_requirements(missing)
                    self.runtime.install_requirements()
                    continue

            self.log(f"ðŸ’¥ Runtime Error: {log[:100]}...")
            
            # ãƒ’ãƒ¼ãƒªãƒ³ã‚° (1å›žå‹è² )
            content = self.ws.project_files[entry]
            _, fixed, strat = self.healer.heal(entry, content, [log], self.ws.project_files, kit)
            
            if strat not in ["Loop_Ignored", "Skipped"]:
                self.ws.save_file(entry, fixed)
                self.ws.commit(f"Runtime Fix {entry}")
            else:
                self.log("âš ï¸ Runtime fix skipped.")
                break

    def _final_audit(self):
        """æœ€çµ‚ QA ç›£æŸ»ï¼ˆ1å›žï¼‰"""
        self.log("ðŸ•µï¸ Final QA Audit")
        res = self.qa.audit_and_fix(self.ws.project_files)
        
        if res:
            # LLMã®å‡ºåŠ›ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ä¿å­˜
            self.ws.parse_and_save_files(res)
            self.ws.commit("QA Fix")
            self.log("âœ¨ QA Fixed files")

    def _extract_module(self, log):
        import re
        m = re.search(r"No module named ['\"]([^'\"]+)['\"]", log)
        return m.group(1).split('.')[0] if m else None

    # --- Special Modes ---
    def _mode_kit_gen(self, prompt):
        yaml = self.kit_gen.generate_kit(prompt)
        name = self.kit_mgr.save_new_kit(yaml)
        return {"success": True, "logs": self.logs + [f"Kit {name} created."]}

    def _mode_research(self, prompt):
        rep = self.search.research(prompt)
        self.ws.save_file("research_report.md", rep)
        return {"success": True, "logs": self.logs + ["Research done."], "files": self.ws.project_files}


# --- Entry Point ---

def run_agent_task(prompt):
    """å¤–éƒ¨APIã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    orchestrator = Orchestrator()
    try: 
        return orchestrator.run(prompt)
    except Exception as e: 
        logger.error(f"Err: {e}")
        return {"success": False, "error": str(e), "logs": orchestrator.logs}
    finally: 
        orchestrator.cleanup()

def get_realtime_data(start=0):
    # å®Ÿè¡Œç’°å¢ƒå¤–ã§ã¯ãƒ­ã‚°å–å¾—ã¯æ©Ÿèƒ½ã—ãªã„ãŸã‚ã€ãƒ€ãƒŸãƒ¼ã‚’è¿”ã™
    return {"new_logs": [], "stats": {}}


## File: `requirements.txt`

txt
fastapi
uvicorn
pydantic
python-dotenv
google-generativeai
pyyaml
autopep8
beautifulsoup4
requests
ddgs
pandas
plotly
streamlit


## File: `server.py`

py
import os
import uvicorn
import logging
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
import sys

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from agent_core import run_agent_task
from src.config import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("EvoAPI")

app = FastAPI(title="Evo Studio API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

os.makedirs("templates", exist_ok=True)
os.makedirs("static", exist_ok=True)
# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã®å¯¾ç­–
os.makedirs(config.OUTPUT_DIR, exist_ok=True)

app.mount("/static", StaticFiles(directory="static"), name="static")
# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã«ãƒžã‚¦ãƒ³ãƒˆ
app.mount("/preview", StaticFiles(directory=config.OUTPUT_DIR), name="preview")

class PromptRequest(BaseModel):
    prompt: str

@app.get("/")
async def index():
    if os.path.exists("templates/index.html"):
        return FileResponse("templates/index.html")
    return {"message": "Welcome to Evo API. Please create templates/index.html"}

@app.post("/generate")
async def generate(req: PromptRequest):
    result = run_agent_task(req.prompt)
    
    # æˆåŠŸãƒ•ãƒ©ã‚°ãŒFalseã§ã‚‚ã€æˆæžœç‰©(files)ãŒã‚ã‚‹å ´åˆã¯ã€Œéƒ¨åˆ†çš„æˆåŠŸã€ã¨ã—ã¦è¿”ã™
    if not result["success"]:
        if result.get("files"):
            # ã‚¨ãƒ©ãƒ¼ã¯ã‚ã‚‹ãŒãƒ•ã‚¡ã‚¤ãƒ«ã¯ç”Ÿæˆã•ã‚ŒãŸå ´åˆ
            result["success"] = True
            result["warning"] = result.get("error")
            del result["error"]
        else:
            return JSONResponse(content=result, status_code=200)
            
    return result

@app.get("/files")
async def list_files():
    files = []
    IGNORE_DIRS = {".git", "__pycache__", ".venv", "node_modules", "venv", "_trash"}
    # éš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ä¸è¦ãªæ‹¡å¼µå­ã‚’é™¤å¤–
    IGNORE_EXTS = {".pyc", ".pyo", ".pyd", ".DS_Store", ".db", ".sqlite", ".png", ".jpg", ".jpeg", ".ico"}

    if not os.path.exists(config.OUTPUT_DIR):
        return {"files": []}

    for root, dirs, filenames in os.walk(config.OUTPUT_DIR):
        dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]
        
        for filename in filenames:
            _, ext = os.path.splitext(filename)
            if ext in IGNORE_EXTS: continue
                
            rel_path = os.path.relpath(os.path.join(root, filename), config.OUTPUT_DIR)
            files.append(rel_path.replace("\\", "/"))
            
    return {"files": files}

@app.get("/files/content")
async def get_file_content(filename: str):
    path = os.path.join(config.OUTPUT_DIR, filename)
    
    # ãƒ‘ã‚¹ãƒ»ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«å¯¾ç­–
    if not os.path.abspath(path).startswith(os.path.abspath(config.OUTPUT_DIR)):
        raise HTTPException(status_code=403, detail="Access denied")
        
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="File not found")

    try:
        with open(path, 'r', encoding='utf-8') as f:
            return {"content": f.read()}
    except UnicodeDecodeError:
        return {"content": "(Binary file)"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5000)


## File: `evo_output\ai_profiler_service.py`

py
import os
import streamlit as st
import google.generativeai as genai

class AIProfilerService:
    """
    A service class to interact with the Google Gemini AI model for
    generating analysis of development styles based on repository data.
    """
    def __init__(self, user_key=None):
        """
        Initializes the Gemini API configuration and model.
        Strictly requires a user-provided key. Does NOT fallback to environment variables
        to prevent accidental usage of the developer's quota.
        """
        # Strictly use the key provided by the user via the UI
        api_key = user_key

        # Check if the API key is provided
        if not api_key:
            st.error("Gemini API Key is missing. Please enter it in the sidebar settings.")
            st.stop()  # Halt the Streamlit application if the key is missing
        
        # Configure the generative AI library with the retrieved API key
        genai.configure(api_key=api_key)
        
        # Initialize the Gemini model.
        # 'gemini-1.5-flash' is chosen for its efficiency, speed, and capability
        self.model = genai.GenerativeModel('gemini-2.5-flash-lite')

    # @st.cache_data(ttl=3600, show_spinner="Getting AI analysis from Gemini...")
    def get_ai_analysis(self, prompt_text: str) -> str:
        """
        Calls the configured Google Gemini model to generate a comprehensive analysis
        of the development style based on the provided detailed prompt text.
        """
        try:
            # Generate content using the initialized Gemini model
            response = self.model.generate_content(prompt_text)
            
            # Validate the response from Gemini to ensure it contains content
            if not response.candidates:
                return "Gemini returned no valid candidates for analysis. " \
                       "This might indicate an issue with the prompt or the model's response."
            
            # Extract and return the generated text from the first candidate
            return response.text
        except Exception as e:
            st.error(f"An error occurred while calling the Gemini API: {e}")
            return "Failed to retrieve AI analysis due to an internal error or API issue. " \
                   "Please verify your API key, network connection, and try again."


## File: `evo_output\app.py`

py
import streamlit as st
import os
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import json
import logging
import re
from collections import defaultdict
import shutil
import tempfile
import subprocess

# Import services
from git_miner_service import mine_git_repository
from data_analyzer_service import DataAnalyzerService
from ai_profiler_service import AIProfilerService

# For tenacity (retry logic)
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Plotting Functions ---

def plot_monthly_commits(monthly_commit_data):
    """Plots the number of commits per month."""
    if monthly_commit_data.empty:
        st.write("No commit data available to plot monthly commits.")
        return None

    fig = px.bar(
        monthly_commit_data,
        x='Month',
        y='Commits',
        title='Monthly Commit Activity',
        labels={'Commits': 'Number of Commits', 'Month': 'Month'},
        hover_data={'Month': '|%Y-%m'},
        color_discrete_sequence=px.colors.qualitative.Plotly
    )
    fig.update_xaxes(tickformat='%Y-%m')
    fig.update_layout(xaxis_tickangle=-45)
    return fig

def plot_activity_heatmap(activity_heatmap_data):
    """Plots a heatmap of commit activity by hour and weekday."""
    if activity_heatmap_data.empty:
        st.write("No activity heatmap data available.")
        return None

    weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    
    all_hours = range(24)
    df_full = pd.DataFrame([(d, h) for d in weekdays for h in all_hours], columns=['Day of Week', 'Hour of Day'])
    activity_heatmap_data_merged = pd.merge(df_full, activity_heatmap_data, on=['Day of Week', 'Hour of Day'], how='left').fillna(0)
    activity_heatmap_data_merged['Commits'] = activity_heatmap_data_merged['Commits'].astype(int)

    activity_heatmap_data_merged['Day of Week'] = pd.Categorical(
        activity_heatmap_data_merged['Day of Week'],
        categories=weekdays,
        ordered=True
    )
    activity_heatmap_data_merged = activity_heatmap_data_merged.sort_values(['Day of Week', 'Hour of Day'])

    fig = px.density_heatmap(
        activity_heatmap_data_merged,
        x='Hour of Day',
        y='Day of Week',
        z='Commits',
        title='Commit Activity Heatmap (Hour of Day vs. Day of Week)',
        labels={'Hour of Day': 'Hour of Day', 'Day of Week': 'Day of Week', 'Commits': 'Number of Commits'},
        color_continuous_scale="Viridis",
        category_orders={"Day of Week": weekdays}
    )
    fig.update_xaxes(side="top", tickvals=list(range(24)))
    fig.update_layout(yaxis_autorange="reversed")
    return fig

def plot_file_extension_changes(file_extension_data):
    """Plots the distribution of file extension changes."""
    if file_extension_data.empty:
        st.write("No file extension data available.")
        return None

    fig = px.bar(
        file_extension_data.head(10),
        x='Extension',
        y='Changes',
        title='File Extension Total Changes (Top 10)',
        labels={'Extension': 'File Extension', 'Changes': 'Total Lines Changed'},
        color_discrete_sequence=px.colors.qualitative.Plotly
    )
    fig.update_layout(xaxis_tickangle=-45)
    return fig

def plot_file_churn_ranking(file_churn_data):
    """Plots a bar chart of the top N files by churn."""
    if file_churn_data.empty:
        st.write("No file churn data available.")
        return None

    max_files = min(20, len(file_churn_data))
    if max_files == 0:
        st.write("No file churn data available to display.")
        return None
    
    top_n = st.slider(
        "Number of files to show in Churn Ranking:",
        min_value=5,
        max_value=max_files,
        value=min(10, max_files)
    )
    display_data = file_churn_data.head(top_n)

    fig = px.bar(
        display_data,
        x='churn_count',
        y='file_path',
        orientation='h',
        title=f'Top {top_n} Files by Churn (Most Frequent Changes)',
        labels={'churn_count': 'Number of Commits Affecting File', 'file_path': 'File Path'},
        color_discrete_sequence=px.colors.qualitative.Vivid,
        height=min(600, 50 * top_n + 150)
    )
    fig.update_layout(yaxis={'categoryorder':'total ascending'})
    return fig

# --- Analysis Orchestration Function ---

# @st.cache_data(show_spinner=False)
def run_analysis(repo_url, repo_path, progress_callback, api_key):
    """
    Orchestrates the git mining, data analysis, and AI profiling.
    Uses a progress callback for Streamlit.
    """
    # Pass the user's API key to the service
    ai_profiler = AIProfilerService(user_key=api_key)
    
    results = {}
    temp_repo_dir = None

    try:
        current_repo_path = repo_path
        
        if repo_url and not repo_path:
            progress_callback(5, f"Cloning repository from {repo_url}...")
            logger.info(f"Cloning GitHub repository: {repo_url}")
            temp_repo_dir = tempfile.mkdtemp()
            
            try:
                subprocess.run(['git', 'clone', '--depth', '100', repo_url, temp_repo_dir], check=True)
                current_repo_path = temp_repo_dir
                logger.info(f"Repository cloned to temporary directory: {current_repo_path}")
            except subprocess.CalledProcessError as e:
                st.error(f"Failed to clone repository from URL: {repo_url}. Please ensure the URL is correct and the repository is public. Error: {e}")
                logger.error(f"Failed to clone repository: {e}", exc_info=True)
                return None
            except Exception as e:
                st.error(f"An unexpected error occurred during cloning: {e}")
                logger.error(f"Cloning failed: {e}", exc_info=True)
                return None
        
        if not current_repo_path:
            st.error("No valid repository path determined for analysis.")
            return None

        # Step 1: Mine Git Repository
        progress_callback(10, "Mining Git Repository...")
        logger.info(f"Mining repository: Path={current_repo_path}")
        unique_commits_df, file_modifications_df = mine_git_repository(
            repo_path=current_repo_path,
            progress_callback=lambda current, total, msg: progress_callback(10 + int(current/total*20), msg)
        )
        
        if unique_commits_df.empty:
            st.error("No commit data found for the provided repository.")
            return None

        analyzer = DataAnalyzerService(unique_commits_df, file_modifications_df)

        # Step 2-6: Prepare Data
        progress_callback(35, "Analyzing Monthly Commits...")
        monthly_commit_data = analyzer.prepare_monthly_commit_data()
        results['monthly_commit_data'] = monthly_commit_data

        progress_callback(45, "Analyzing Commit Activity Heatmap...")
        activity_heatmap_data = analyzer.prepare_activity_heatmap_data()
        results['activity_heatmap_data'] = activity_heatmap_data

        progress_callback(60, "Analyzing File Extension Changes...")
        file_extension_data = analyzer.prepare_file_extension_data()
        results['file_extension_data'] = file_extension_data

        progress_callback(75, "Analyzing File Churn Ranking...")
        file_churn_data = analyzer.prepare_file_churn_ranking_data()
        results['file_churn_data'] = file_churn_data

        progress_callback(85, "Generating Analysis Summary for AI...")
        analysis_summary_dict = analyzer.generate_analysis_summary()
        analysis_summary = analysis_summary_dict['summary_text']
        results['analysis_summary'] = analysis_summary
        
        # Step 7: Get AI Analysis with Retry Logic
        progress_callback(90, "Calling AI for deep analysis (this may take a minute)...")
        logger.info("Calling AI profiler service...")

        # Update prompt for better actionable advice (English Version)
        enhanced_prompt = f"""
        Based on the following Git repository analysis summary, please provide a "Development Style Diagnosis" and "Concrete Advice".
        
        Analysis Summary:
        {analysis_summary}

        Please output in the following format (English):
        
        ### Development Style Diagnosis: ã€(Catchy Title)ã€‘
        (Description of the style based on data)

        ### Practical Advice for Improvement
        1. **[Point 1]**: [Actionable advice]
        2. **[Point 2]**: [Actionable advice]
        3. **[Point 3]**: [Actionable advice]
        """

        ai_analysis_with_retry = retry(
            stop=stop_after_attempt(3),
            wait=wait_exponential(multiplier=1, min=2, max=10),
            retry=retry_if_exception_type(Exception),
            reraise=True,
            before_sleep=lambda retry_state: logger.warning(
                f"Retrying AI analysis (attempt {retry_state.attempt_number}/{retry_state.max_attempts_reached + 1})..."
            )
        )(ai_profiler.get_ai_analysis)

        try:
            ai_summary_results = ai_analysis_with_retry(enhanced_prompt)
            results['ai_summary_results'] = ai_summary_results
        except Exception as e:
            logger.error(f"Failed to get AI analysis after multiple retries: {e}", exc_info=True)
            st.error(f"Failed to get AI analysis. Please check your API key. Error: {e}")
            results['ai_summary_results'] = "AI analysis failed."

        progress_callback(100, "Analysis complete!")
        return results

    except Exception as e:
        logger.error(f"An unexpected error occurred during analysis: {e}", exc_info=True)
        st.error(f"An unexpected error occurred during analysis: {e}")
        return None
    finally:
        if temp_repo_dir and os.path.exists(temp_repo_dir):
            try:
                shutil.rmtree(temp_repo_dir, ignore_errors=True)
                logger.info(f"Cleaned up temporary cloned repository at: {temp_repo_dir}")
            except Exception as e:
                logger.error(f"Error cleaning up temporary repository {temp_repo_dir}: {e}")

# --- Main Streamlit Application ---

def main():
    st.set_page_config(layout="wide", page_title="Git Repository AI Profiler")

    st.title("ðŸ¤– Git Repository AI Profiler")

    with st.sidebar:
        st.header("Settings")
        
        # --- API Key Input ---
        user_api_key = st.text_input("Enter your Gemini API Key", type="password", help="Get your key from https://aistudio.google.com/app/apikey")
        
        st.markdown("---")
        
        st.header("Repository Input")
        repo_option = st.radio(
            "Select repository source:",
            ("Enter GitHub URL", "Enter Local Path")
        )

        repo_path_for_analysis = None
        repo_url_for_analysis = None

        if repo_option == "Enter Local Path":
            local_path_input = st.text_input("Enter path to local Git repository", value=os.getcwd())
            if os.path.isdir(local_path_input) and os.path.exists(os.path.join(local_path_input, '.git')):
                repo_path_for_analysis = local_path_input
                st.success("Valid local Git repository found.")
            elif local_path_input:
                st.warning("Invalid Git repository path.")

        elif repo_option == "Enter GitHub URL":
            github_url = st.text_input("Enter GitHub repository URL", value="https://github.com/streamlit/streamlit")
            if github_url:
                repo_url_for_analysis = github_url

        st.subheader("Analysis Controls")
        
        # Check if API Key is present
        # Strictly require user input, ignoring environment variables to prevent usage of the developer's key
        has_api_key = bool(user_api_key)
        
        if st.button("Analyze Repository", type="primary", disabled=not (bool(repo_path_for_analysis) or bool(repo_url_for_analysis))):
            if not has_api_key:
                st.error("ðŸ”’ Please enter YOUR Gemini API Key in the sidebar. This app requires your own key to function.")
            else:
                st.session_state['run_analysis'] = True
                st.session_state['repo_path_param'] = repo_path_for_analysis
                st.session_state['repo_url_param'] = repo_url_for_analysis
                st.session_state['user_api_key'] = user_api_key # Store for this session
                st.session_state['analysis_results'] = None
        
        if st.button("Clear Cache & Reset"):
            st.cache_data.clear()
            st.session_state.clear()
            st.rerun()

    # --- Main Content Area ---
    if 'run_analysis' in st.session_state and st.session_state['run_analysis']:
        # Double check API key before running
        current_api_key = st.session_state.get('user_api_key')
        
        display_repo_info = st.session_state.get('repo_url_param') or st.session_state.get('repo_path_param')
        st.info(f"Starting analysis for repository: {display_repo_info}")
        
        progress_text_placeholder = st.empty()
        progress_bar_placeholder = st.progress(0)

        def update_progress_ui(percent_complete, message):
            progress_bar_placeholder.progress(int(percent_complete) / 100)
            progress_text_placeholder.text(f"Progress: {message} ({int(percent_complete)}%)")

        results = run_analysis(
            repo_url=st.session_state.get('repo_url_param'),
            repo_path=st.session_state.get('repo_path_param'),
            progress_callback=update_progress_ui,
            api_key=current_api_key # Pass the key
        )
        st.session_state['analysis_results'] = results
        st.session_state['run_analysis'] = False
        st.rerun()

    if 'analysis_results' in st.session_state and st.session_state['analysis_results'] is not None:
        results = st.session_state['analysis_results']

        st.success("Analysis Complete!")

        st.subheader("AI-Powered Repository Insights")
        if 'ai_summary_results' in results and results['ai_summary_results'] and results['ai_summary_results'] != "AI analysis failed.":
            st.markdown(results['ai_summary_results'])
        else:
            st.warning("AI analysis results are not available.")

        st.subheader("Detailed Repository Metrics")
        tab_titles = ["Monthly Commits", "Activity Heatmap", "File Extension Changes", "File Churn Ranking"]
        tabs = st.tabs(tab_titles)

        with tabs[0]:
            if 'monthly_commit_data' in results:
                st.plotly_chart(plot_monthly_commits(results['monthly_commit_data']), use_container_width=True)
        with tabs[1]:
            if 'activity_heatmap_data' in results:
                st.plotly_chart(plot_activity_heatmap(results['activity_heatmap_data']), use_container_width=True)
        with tabs[2]:
            if 'file_extension_data' in results:
                st.plotly_chart(plot_file_extension_changes(results['file_extension_data']), use_container_width=True)
        with tabs[3]:
            if 'file_churn_data' in results:
                st.plotly_chart(plot_file_churn_ranking(results['file_churn_data']), use_container_width=True)

    else:
        st.info("ðŸ‘ˆ Enter your Gemini API Key and Repository details in the sidebar to start.")

if __name__ == "__main__":
    main()


## File: `evo_output\data_analyzer_service.py`

py
import pandas as pd
from datetime import datetime
import os

# Helper function (private to this module)
def _get_file_extension(filepath: str) -> str:
    """Extracts the file extension from a given file path.
    Handles None/NaN paths and returns 'no_extension' for consistency.
    Also handles files like .gitignore correctly.
    """
    if pd.isna(filepath) or not isinstance(filepath, str):
        return 'no_extension'
    # Use os.path.splitext, then ensure it's lower case.
    # If no extension (e.g., 'file' or '.gitignore'), ext will be empty or '.gitignore' itself.
    base, ext = os.path.splitext(filepath)
    if not ext and base and base.startswith('.'): # Handles files like '.gitignore' or '.env'
        return base.lower()
    return ext.lower() if ext else 'no_extension'

class DataAnalyzerService:
    """
    Service class responsible for processing raw Git commit data
    into aggregated and summarized formats suitable for visualization
    and AI analysis.
    """

    def __init__(self, unique_commits_df: pd.DataFrame, file_modifications_df: pd.DataFrame):
        """
        Initializes the DataAnalyzerService with the raw commit and file modification data.

        Args:
            unique_commits_df: DataFrame with unique commit-level data from git_miner_service.
            file_modifications_df: DataFrame with file-level modification data from git_miner_service.
        """
        # Ensure unique_commits_df is not empty before processing
        if unique_commits_df.empty:
            self.unique_commits_df = pd.DataFrame(columns=[
                'hash', 'author_date', 'author_name', 'insertions', 'deletions',
                'lines_added_commit', 'lines_deleted_commit' # Ensure these columns are present for summary
            ])
        else:
            self.unique_commits_df = unique_commits_df.copy()
            # Ensure author_date is datetime. Convert timezone-aware to timezone-naive for consistent calculations.
            # This makes dt.weekday, dt.hour, to_period('M') behave predictably without timezone complications.
            self.unique_commits_df['author_date'] = pd.to_datetime(self.unique_commits_df['author_date']).dt.tz_localize(None)

        # Ensure file_modifications_df is not empty before processing
        if file_modifications_df.empty:
            self.file_modifications_df = pd.DataFrame(columns=[
                'commit_hash', 'change_type', 'file_path', 'lines_added', 'lines_deleted', 'extension'
            ])
        else:
            self.file_modifications_df = file_modifications_df.copy()
            # Apply robust file extension extraction using the internal helper for consistency
            self.file_modifications_df['extension'] = self.file_modifications_df['file_path'].apply(_get_file_extension)


    def prepare_monthly_commit_data(self) -> pd.DataFrame:
        """
        Prepares data for monthly commit count trend visualization, counting unique commits.

        Returns:
            DataFrame with 'Month' (datetime) and 'Commits' (count).
        """
        if self.unique_commits_df.empty:
            return pd.DataFrame(columns=['Month', 'Commits'])
        
        # Group by year and month from unique commits and count
        monthly_commits = self.unique_commits_df.groupby(
            self.unique_commits_df['author_date'].dt.to_period('M')
        ).size().reset_index(name='Commits')
        
        # Convert Period to datetime for easier plotting with Plotly
        monthly_commits['Month'] = monthly_commits['author_date'].dt.to_timestamp()
        
        return monthly_commits[['Month', 'Commits']].sort_values('Month')

    def prepare_activity_heatmap_data(self) -> pd.DataFrame:
        """
        Prepares data for activity heatmap (Day of Week vs. Hour of Day), counting unique commits.

        Returns:
            DataFrame with 'Hour of Day', 'Day of Week', 'Commits'. This is in 'long' format.
        """
        weekday_names = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
        
        if self.unique_commits_df.empty:
            # Return an empty heatmap structure consistent with plot_activity_heatmap's expectation (long format)
            return pd.DataFrame(columns=['Hour of Day', 'Day of Week', 'Commits'])

        # Extract weekday (0=Monday, 6=Sunday) and hour from unique commits
        self.unique_commits_df['weekday'] = self.unique_commits_df['author_date'].dt.weekday
        self.unique_commits_df['hour'] = self.unique_commits_df['author_date'].dt.hour

        # Group by weekday and hour, then count unique commits
        activity_counts = self.unique_commits_df.groupby(['weekday', 'hour']).size().reset_index(name='Commits')

        # Create a full grid to ensure all hours and weekdays are represented, filling missing with 0
        all_hours = pd.RangeIndex(start=0, stop=24)
        all_weekdays_nums = pd.RangeIndex(start=0, stop=7)
        full_grid = pd.MultiIndex.from_product([all_weekdays_nums, all_hours], names=['weekday', 'hour']).to_frame(index=False)
        
        # Merge with actual activity counts
        heatmap_df = pd.merge(full_grid, activity_counts, on=['weekday', 'hour'], how='left').fillna(0)
        
        # Map weekday numbers to names and rename hour column
        heatmap_df['Day of Week'] = heatmap_df['weekday'].map(lambda x: weekday_names[x])
        heatmap_df['Hour of Day'] = heatmap_df['hour']

        return heatmap_df[['Hour of Day', 'Day of Week', 'Commits']].astype({'Commits': int})


    def prepare_file_extension_data(self) -> pd.DataFrame:
        """
        Prepares data for file extension changes visualization.
        Counts total lines changed (added + deleted) per file extension.

        Returns:
            DataFrame with 'Extension', 'Changes' (total lines changed).
        """
        if self.file_modifications_df.empty:
            return pd.DataFrame(columns=['Extension', 'Changes'])
        
        # The file_modifications_df already has 'extension', 'lines_added', 'lines_deleted' for each file modification
        # Sum file-level added and deleted lines for each modification
        # Ensure columns exist and are numeric, default to 0 if not present or non-numeric
        lines_added = pd.to_numeric(self.file_modifications_df.get('lines_added', 0), errors='coerce').fillna(0)
        lines_deleted = pd.to_numeric(self.file_modifications_df.get('lines_deleted', 0), errors='coerce').fillna(0)
        
        self.file_modifications_df['file_changes_lines'] = lines_added + lines_deleted
        
        # Group by extension and sum the changes
        extension_summary = self.file_modifications_df.groupby('extension')['file_changes_lines'].sum().reset_index(name='Changes')

        # Rename columns for clarity in plots
        extension_summary.rename(columns={'extension': 'Extension'}, inplace=True)
        
        # Sort by changes
        extension_summary = extension_summary.sort_values(by='Changes', ascending=False)

        return extension_summary

    def prepare_file_churn_ranking_data(self) -> pd.DataFrame:
        """
        Calculates file churn, ranking files by the number of unique commits they appear in.

        Returns:
            DataFrame with 'file_path' and 'churn_count'.
        """
        if self.file_modifications_df.empty:
            return pd.DataFrame(columns=['file_path', 'churn_count'])

        # Group by file_path and count unique commit_hashes for each file
        file_churn = self.file_modifications_df.groupby('file_path')['commit_hash'].nunique().reset_index(name='churn_count')
        
        # Sort by churn count in descending order
        file_churn = file_churn.sort_values(by='churn_count', ascending=False)

        return file_churn


    def generate_analysis_summary(self) -> dict:
        """
        Generates a summary dictionary of key project metrics for Gemini analysis.

        Returns:
            A dictionary containing key summary statistics and a narrative summary text.
        """
        if self.unique_commits_df.empty:
            return {
                "total_commits": 0,
                "project_duration_days": 0,
                "first_commit_date": "N/A",
                "last_commit_date": "N/A",
                "total_authors": 0,
                "top_author": "N/A",
                "top_author_commits": 0,
                "avg_commits_per_day": 0.0,
                "total_lines_added": 0,
                "total_lines_deleted": 0,
                "most_active_weekday": "N/A",
                "most_active_hour": "N/A",
                "dominant_file_extensions": [],
                "top_churned_files": [],
                "summary_text": "No commit data available for analysis. Please provide a valid Git repository URL with commits."
            }

        total_commits = len(self.unique_commits_df)
        first_commit_date = self.unique_commits_df['author_date'].min()
        last_commit_date = self.unique_commits_df['author_date'].max()
        project_duration_days = (last_commit_date - first_commit_date).days if total_commits > 1 else 0

        total_authors = self.unique_commits_df['author_name'].nunique()
        author_counts = self.unique_commits_df['author_name'].value_counts()
        top_author = author_counts.index[0] if not author_counts.empty else "N/A"
        top_author_commits = int(author_counts.iloc[0]) if not author_counts.empty else 0

        avg_commits_per_day = total_commits / (project_duration_days + 1) if project_duration_days >= 0 else 0
        avg_commits_per_day = round(avg_commits_per_day, 2)

        # Use commit-level 'lines_added_commit' and 'lines_deleted_commit' for total lines changed
        # Ensure columns exist and are numeric, default to 0 if not present or non-numeric
        total_lines_added = pd.to_numeric(self.unique_commits_df.get('lines_added_commit', 0), errors='coerce').fillna(0).sum()
        total_lines_deleted = pd.to_numeric(self.unique_commits_df.get('lines_deleted_commit', 0), errors='coerce').fillna(0).sum()


        # Get activity heatmap data to find most active weekday/hour
        heatmap_data = self.prepare_activity_heatmap_data() # Call internal method
        most_active_weekday = "N/A"
        most_active_hour = "N/A"
        if not heatmap_data.empty and heatmap_data['Commits'].sum() > 0:
            # Find the row with maximum 'Commits'
            max_activity_row = heatmap_data.loc[heatmap_data['Commits'].idxmax()]
            most_active_weekday = max_activity_row['Day of Week']
            most_active_hour = int(max_activity_row['Hour of Day'])


        # Get file extension data
        file_extension_summary = self.prepare_file_extension_data() # Call internal method
        dominant_file_extensions = file_extension_summary.head(3)['Extension'].tolist() if not file_extension_summary.empty else []

        # Get file churn data
        file_churn_summary = self.prepare_file_churn_ranking_data()
        top_churned_files = file_churn_summary.head(3)['file_path'].tolist() if not file_churn_summary.empty else []


        # Construct summary text
        summary_lines = []
        summary_lines.append(f"This repository contains {total_commits} unique commits.")
        if total_commits > 0:
            summary_lines.append(f"It spans {project_duration_days} days, from {first_commit_date.strftime('%Y-%m-%d')} to {last_commit_date.strftime('%Y-%m-%d')}.")
            summary_lines.append(f"There are {total_authors} unique authors. The most active author is '{top_author}' with {top_author_commits} commits.")
            summary_lines.append(f"On average, {avg_commits_per_day} commits are made per day.")
            summary_lines.append(f"A total of {int(total_lines_added)} lines were added and {int(total_lines_deleted)} lines were deleted across all unique commits.")
            if most_active_weekday != "N/A":
                summary_lines.append(f"The peak activity time is typically on {most_active_weekday} around {most_active_hour}:00.")
            if dominant_file_extensions:
                summary_lines.append(f"Dominant file types changed include: {', '.join(dominant_file_extensions)}.")
            else:
                summary_lines.append("No specific dominant file extensions were identified.")
            if top_churned_files:
                summary_lines.append(f"Top churned files (frequently changed) include: {', '.join(top_churned_files)}.")
            else:
                summary_lines.append("No specific churned files were identified.")
        
        summary_text = " ".join(summary_lines)

        summary_data = {
            "total_commits": total_commits,
            "project_duration_days": project_duration_days,
            "first_commit_date": str(first_commit_date.strftime('%Y-%m-%d %H:%M:%S')), # Format for consistent JSON
            "last_commit_date": str(last_commit_date.strftime('%Y-%m-%d %H:%M:%S')),   # Format for consistent JSON
            "total_authors": total_authors,
            "top_author": top_author,
            "top_author_commits": top_author_commits,
            "avg_commits_per_day": avg_commits_per_day,
            "total_lines_added": int(total_lines_added), # Ensure int for JSON serialization
            "total_lines_deleted": int(total_lines_deleted), # Ensure int for JSON serialization
            "most_active_weekday": most_active_weekday,
            "most_active_hour": most_active_hour,
            "dominant_file_extensions": dominant_file_extensions,
            "top_churned_files": top_churned_files, # Added for completeness in summary data
            "summary_text": summary_text
        }
        return summary_data


## File: `evo_output\gemini_service.py`

py
import os
import google.generativeai as genai
from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_exception_type
import logging

# Configure logging for tenacity and Gemini service
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GeminiService:
    """
    A service class to interact with the Google Gemini API,
    incorporating retry logic for robust API calls.
    """

    def __init__(self):
        """
        Initializes the GeminiService by configuring the API key
        and loading the Gemini Pro model.
        Raises a ValueError if the GEMINI_API_KEY environment variable is not set.
        """
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            logger.error("GEMINI_API_KEY environment variable not set. Please set it to use Gemini.")
            raise ValueError("GEMINI_API_KEY environment variable not set.")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-pro')
        logger.info("GeminiService initialized and model 'gemini-pro' loaded.")

    @retry(
        wait=wait_random_exponential(multiplier=1, min=4, max=10), # Exponential backoff with random jitter
        stop=stop_after_attempt(3),                              # Stop after 3 attempts
        retry=retry_if_exception_type(Exception),                 # Retry on any exception
        reraise=True                                             # Re-raise the last exception if all retries fail
    )
    def generate_content_with_retry(self, prompt: str) -> str:
        """
        Sends a prompt to the Gemini API and retrieves the generated content,
        with built-in retry logic using tenacity.

        Args:
            prompt: The text prompt to send to the Gemini model.

        Returns:
            The generated text content from the Gemini model.

        Raises:
            Exception: If the Gemini API call fails after all retries,
                       or if the response content is empty/invalid.
        """
        logger.info(f"Attempting Gemini content generation (attempt {self.generate_content_with_retry.retry.statistics['attempts'] + 1} of 3). Prompt snippet: {prompt[:100]}...")
        try:
            response = self.model.generate_content(prompt)

            # Check if the response contains valid parts and text
            if not response.parts or not response.text:
                logger.warning(f"Gemini API returned an empty or invalid response. Response: {response}")
                # Raise an error to trigger a retry if applicable
                raise ValueError("Gemini API returned no content or invalid response.")

            logger.info("Gemini content generation successful.")
            return response.text
        except Exception as e:
            logger.error(f"Gemini API call failed with error: {e}")
            # The 'reraise=True' in @retry decorator will handle re-raising after all attempts.
            # We just need to ensure an exception is raised here to signal failure to tenacity.
            raise


## File: `evo_output\git_miner_service.py`

py
import os
from pydriller import Repository, Git
import pandas as pd
from datetime import datetime

def mine_git_repository(repo_path: str, progress_callback=None) -> (pd.DataFrame, pd.DataFrame):
    """
    Mines a Git repository to extract commit and file change data.

    This function iterates through all commits in a specified Git repository,
    collecting detailed information about each commit and the files modified
    within them. It can optionally report progress via a callback function,
    which is useful for UI updates (e.g., Streamlit progress bars).

    Args:
        repo_path (str): The absolute or relative path to the Git repository.
                         This path should point to the root directory of the repository.
        progress_callback (callable, optional): A function to call with progress updates.
                                                If provided, it should accept three arguments:
                                                (current_step: int, total_steps: int, message: str).
                                                Defaults to None, meaning no progress reporting.

    Returns:
        tuple: A tuple containing two pandas DataFrames:
               - commits_df (pd.DataFrame): DataFrame of commit-level data, with columns
                                            like 'hash', 'author_name', 'author_date', 'message',
                                            'lines_added_commit', 'lines_deleted_commit',
                                            'files_changed_commit'.
               - file_changes_df (pd.DataFrame): DataFrame of file-level change data, with columns
                                                 like 'commit_hash', 'change_type', 'old_path',
                                                 'new_path', 'file_path', 'lines_added',
                                                 'lines_deleted', 'nloc', 'complexity'.

    Raises:
        FileNotFoundError: If the specified repository path does not exist.
        ValueError: If the specified path is not a valid Git repository.
    """
    if not os.path.exists(repo_path):
        raise FileNotFoundError(f"Repository path does not exist: {repo_path}")
    if not os.path.isdir(os.path.join(repo_path, '.git')):
        raise ValueError(f"'{repo_path}' is not a valid Git repository.")

    commits_data = []
    file_changes_data = []

    # Attempt to get the total number of commits for an accurate progress bar
    try:
        git_helper = Git(repo_path)
        total_commits = git_helper.total_commits()
    except Exception as e:
        # Fallback if `Git().total_commits()` fails (e.g., repository corrupted, no commits)
        print(f"Warning: Could not get total commit count for repository {repo_path}: {e}")
        total_commits = 0  # Indicate unknown total

    current_commit_count = 0

    # Initialize Repository miner
    repo_miner = Repository(repo_path)

    for commit in repo_miner.traverse_commits():
        current_commit_count += 1

        # Report progress if a callback is provided
        if progress_callback:
            # If total_commits is unknown (0), we estimate total to prevent ZeroDivisionError
            # and allow the progress bar to show relative movement.
            display_total = total_commits if total_commits > 0 else current_commit_count + 1
            progress_callback(
                current_commit_count,
                display_total,
                f"Mining commit {commit.hash[:7]} by {commit.author.name}"
            )

        # Collect commit-level data
        commits_data.append({
            'hash': commit.hash,
            'author_name': commit.author.name,
            'author_email': commit.author.email,
            'author_date': commit.author_date,
            'committer_name': commit.committer.name,
            'committer_email': commit.committer.email,
            'committer_date': commit.committer_date,
            'message': commit.msg,
            'lines_added_commit': commit.insertions,
            'lines_deleted_commit': commit.deletions,
            'files_changed_commit': len(commit.modified_files)
        })

        # Collect file-level change data for each modification in the commit
        for mod in commit.modified_files:
            file_changes_data.append({
                'commit_hash': commit.hash,
                'change_type': mod.change_type.name,  # e.g., ADD, DELETE, MODIFY, RENAME, COPY
                'old_path': mod.old_path,
                'new_path': mod.new_path,
                # 'file_path' represents the path of the file *after* the change.
                # For deleted files, new_path is None, so old_path is used.
                'file_path': mod.new_path if mod.new_path else mod.old_path,
                'lines_added': mod.added_lines,
                'lines_deleted': mod.deleted_lines,
                # Pydriller can return None for nloc/complexity, default to 0 for int compatibility
                'nloc': mod.nloc if mod.nloc is not None else 0,
                'complexity': mod.complexity if mod.complexity is not None else 0
            })

    # Convert collected data into pandas DataFrames
    commits_df = pd.DataFrame(commits_data)
    file_changes_df = pd.DataFrame(file_changes_data)

    # Post-processing for commits_df
    if not commits_df.empty:
        # Ensure author_date and committer_date are timezone-aware datetime objects, converted to UTC
        commits_df['author_date'] = pd.to_datetime(commits_df['author_date'], utc=True)
        commits_df['committer_date'] = pd.to_datetime(commits_df['committer_date'], utc=True)
    else:
        # Define empty DataFrame with correct dtypes if no commits were found
        commits_df = pd.DataFrame(columns=[
            'hash', 'author_name', 'author_email', 'author_date',
            'committer_name', 'committer_email', 'committer_date', 'message',
            'lines_added_commit', 'lines_deleted_commit', 'files_changed_commit'
        ]).astype({
            'hash': str, 'author_name': str, 'author_email': str,
            'author_date': 'datetime64[ns, UTC]', 'committer_name': str,
            'committer_email': str, 'committer_date': 'datetime64[ns, UTC]',
            'message': str, 'lines_added_commit': int,
            'lines_deleted_commit': int, 'files_changed_commit': int
        })

    # Post-processing for file_changes_df
    if not file_changes_df.empty:
        # Fill potential None values in path columns with empty strings for consistency
        file_changes_df['old_path'] = file_changes_df['old_path'].fillna('').astype(str)
        file_changes_df['new_path'] = file_changes_df['new_path'].fillna('').astype(str)
        file_changes_df['file_path'] = file_changes_df['file_path'].fillna('').astype(str)
    else:
        # Define empty DataFrame with correct dtypes if no file changes were found
        file_changes_df = pd.DataFrame(columns=[
            'commit_hash', 'change_type', 'old_path', 'new_path', 'file_path',
            'lines_added', 'lines_deleted', 'nloc', 'complexity'
        ]).astype({
            'commit_hash': str, 'change_type': str, 'old_path': str,
            'new_path': str, 'file_path': str, 'lines_added': int,
            'lines_deleted': int, 'nloc': int, 'complexity': int
        })

    return commits_df, file_changes_df


## File: `evo_output\requirements.txt`

txt
streamlit
PyDriller
plotly
pandas
google-generativeai
tenacity


## File: `evo_output\æ–°è¦ ãƒ†ã‚­ã‚¹ãƒˆ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ.txt`

txt
# ðŸ¤– Git Repository AI Profiler

[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Powered by Gemini](https://img.shields.io/badge/AI-Gemini%20Flash-8E75B2)](https://deepmind.google/technologies/gemini/)

> **"Not just a log analyzer. It's your AI Career Coach."**
>
> Reveal your coding style, detect burnout risks, and get actionable advice from an AI CTO based on your Git history.

![Demo App Screenshot](https://raw.githubusercontent.com/streamlit/streamlit/develop/docs/_static/logo.png)
*(Replace this link with your actual screenshot or GIF)*

## ðŸ§ What is this?

**Git Repository AI Profiler** is a Streamlit application that mines your Git repository meta-data and uses **Google Gemini 2.5 Flash** to profile your development habits.

Instead of boring statistics, it gives you a **"Developer Persona"** (e.g., *"The Midnight Sprinter"*, *"The Weekend Warrior"*) and provides **concrete, sometimes harsh, advice** to improve your code quality and work-life balance.

## âœ¨ Key Features

* **ðŸ“Š Interactive Visualizations**:
    * **Monthly Commits**: Track your productivity trends.
    * **Activity Heatmap**: Visualize your peak coding hours (Day vs Hour).
    * **Churn Ranking**: Identify "High-Risk Files" that are modified too frequently.
* **ðŸ§  AI-Powered Profiling**:
    * Generates a unique **"Dev Persona"** based on your commit patterns.
    * Provides **CTO-level advice** on refactoring, burnout prevention, and architectural improvements.
* **âš¡ High Performance**:
    * Real-time progress tracking for long mining tasks.
    * Robust API handling with automatic retries.
* **ðŸ“± PWA Ready**:
    * Installable on mobile devices as a Progressive Web App.

## ðŸš€ Quick Start

### 1. Clone the repository
```bash
git clone [https://github.com/YOUR_USERNAME/git-repo-ai-profiler.git](https://github.com/YOUR_USERNAME/git-repo-ai-profiler.git)
cd git-repo-ai-profiler
2. Set up the environment
It is recommended to use a virtual environment.

Bash

# Create virtual environment
python -m venv .venv

# Activate (Windows)
.venv\Scripts\activate

# Activate (Mac/Linux)
source .venv/bin/activate
3. Install dependencies
Bash

pip install -r requirements.txt
4. Set your API Key
You need a Google Gemini API Key. Get it from Google AI Studio.

Windows (Command Prompt):

DOS

set GOOGLE_API_KEY=your_api_key_here
Mac/Linux:

Bash

export GOOGLE_API_KEY="your_api_key_here"
5. Run the App!
Bash

streamlit run app.py
ðŸ› ï¸ Tech Stack
Frontend: Streamlit

Data Processing: Pandas, PyDriller

Visualization: Plotly

AI Engine: Google Gemini API (via google-generativeai)

Resilience: Tenacity

ðŸ¤ Contributing
Contributions are welcome! Please feel free to submit a Pull Request.

Fork the project

Create your Feature Branch (git checkout -b feature/AmazingFeature)

Commit your changes (git commit -m 'Add some AmazingFeature')

Push to the Branch (git push origin feature/AmazingFeature)

Open a Pull Request

ðŸ“„ License
Distributed under the MIT License. See LICENSE for more information.

Created with â¤ï¸ and AI by [Emma Saka]


## File: `kits\Video Content Repurposer Engine.yaml`

yaml
id: "video_content_repurposer"
name: "Video to Blog Engine"
description: "YouTubeå‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºã—ã€Gemini 1.5 Flashã§æ–‡å­—èµ·ã“ã—ã‚’è¡Œã„ã€ãƒ–ãƒ­ã‚°è¨˜äº‹ã¨SNSæŠ•ç¨¿æ–‡ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹Streamlitã‚¢ãƒ—ãƒªã€‚"
version: "1.0.0"

triggers:
  keywords: ["YouTube", "æ–‡å­—èµ·ã“ã—", "è¦ç´„", "ãƒ–ãƒ­ã‚°è‡ªå‹•ç”Ÿæˆ", "Streamlit"]
  sample_prompts:
    - "YouTubeå‹•ç”»ã‚’ãƒ–ãƒ­ã‚°è¨˜äº‹ã«ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’ä½œã£ã¦"
    - "å‹•ç”»ã®URLã‹ã‚‰è¦ç´„ã¨ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ä½œã‚‹ã‚¢ãƒ—ãƒª"

blueprint:
  suggested_tech_stack:
    - "Python 3.10+"
    - "yt-dlp (Video/Audio Downloader)"
    - "google-generativeai (Gemini 2.5 Flash for Transcription & Writing)"
    - "Streamlit (Web UI)"
    - "pydub (Audio Processing)"

  core_components:
    - "Media Service (Download & Convert)"
    - "AI Service (Transcribe & Generate)"
    - "UI Controller (Streamlit View)"
    - "Main Entry Point"

  expected_file_structure:
    - "app.py"             # UIã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    - "media_service.py"   # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»éŸ³å£°å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯
    - "ai_service.py"      # Gemini APIã¨ã®é€šä¿¡ãƒ­ã‚¸ãƒƒã‚¯
    - "utils.py"           # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    - "requirements.txt"
    - "temp/"              # ä¸€æ™‚ä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€

resources:
  domain_knowledge: |
    ã€é–‹ç™ºã®é‰„å‰‡ï¼ˆçµ¶å¯¾åŽ³å®ˆï¼‰ã€‘
    1. **ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¹å‰²åˆ¶é™ (Strict Rule):**
       - `app.py` ã«ã¯ **UIã®è¡¨ç¤ºã‚³ãƒ¼ãƒ‰ï¼ˆãƒœã‚¿ãƒ³ã€ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ãªã©ï¼‰ä»¥å¤–ã‚’æ›¸ã„ã¦ã¯ãªã‚‰ãªã„**ã€‚
       - å…·ä½“çš„ãªå‡¦ç†ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€APIå‘¼ã³å‡ºã—ï¼‰ã¯ã€å¿…ãš `media_service.py` ã‚„ `ai_service.py` ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦å‘¼ã³å‡ºã™ã“ã¨ã€‚
       - ãƒ­ã‚¸ãƒƒã‚¯ã®ãƒ™ã‚¿æ›¸ãã¯ã€Œã‚¹ãƒ‘ã‚²ãƒƒãƒ†ã‚£ã‚³ãƒ¼ãƒ‰ã€ã¨ã¿ãªã—ç¦æ­¢ã™ã‚‹ã€‚
    
    2. **å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†:**
       - å‹•ç”»ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã¯å¤§ãã„ãŸã‚ã€ä¸€åº¦ãƒ­ãƒ¼ã‚«ãƒ«ã® `temp/` ãƒ•ã‚©ãƒ«ãƒ€ã« `.mp3` ã¨ã—ã¦ä¿å­˜ã—ã¦ã‹ã‚‰APIã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹è¨­è¨ˆã«ã™ã‚‹ã“ã¨ã€‚
       - Gemini 2.5 Flash ã¯éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æŽ¥æ‰±ãˆã‚‹ãŸã‚ã€Whisperã§ã¯ãªã **Gemini APIã®File API** ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ï¼ˆå‡¦ç†ãŒé€Ÿãç°¡å˜ï¼‰ã€‚
    
    3. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°:**
       - `yt-dlp` ã¯URLãŒç„¡åŠ¹ã ã¨ã‚¨ãƒ©ãƒ¼ã‚’åããŸã‚ã€`try-except` ã§å›²ã¿ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ†ã‹ã‚Šã‚„ã™ã„ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’UIã«è¡¨ç¤ºã™ã‚‹ã“ã¨ã€‚
    
    4. **ã‚³ã‚¹ãƒˆæ„è­˜:**
       - æ–‡å­—èµ·ã“ã—ã‚„è¦ç´„ã«ã¯ã€å®‰ä¾¡ã§é•·æ–‡ã«å¼·ã„ **Gemini 2.5 Flash** ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã€‚


## File: `kits\autonomous_research_agent.yaml`

yaml
id: "autonomous_research_agent"
name: "Deep Research Agent (Perplexity Style)"
description: "Webæ¤œç´¢ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã€è¦ç´„ã‚’è‡ªå¾‹çš„ã«è¡Œã„ã€å‡ºå…¸ä»˜ãã®èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚"
version: "1.0.0"

triggers:
  keywords: ["ãƒªã‚µãƒ¼ãƒ", "æ¤œç´¢", "èª¿æŸ»", "ãƒ¬ãƒãƒ¼ãƒˆ", "Perplexity"]
  sample_prompts:
    - "æœ€æ–°ã®AIãƒˆãƒ¬ãƒ³ãƒ‰ã«ã¤ã„ã¦èª¿ã¹ã¦ãƒ¬ãƒãƒ¼ãƒˆã«ã—ã¦"
    - "ç«¶åˆä»–ç¤¾ã®å‹•å‘ã‚’èª¿æŸ»ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œã£ã¦"

blueprint:
  suggested_tech_stack:
    - "Python 3.10+"
    - "duckduckgo-search (Search Tool)"
    - "trafilatura (Robust Text Extraction)"
    - "Gemini 2.5 Flash (Reading & Summarizing)"
    - "Gemini 2.5 Pro (Final Report Writing)"
    - "MdUtils (Markdown Generation)"

  core_components:
    - "Search Engine (Query Handler)"
    - "Web Scraper (Content Fetcher)"
    - "Information Synthesizer (LLM)"
    - "Report Builder"
    - "Main Controller"

  expected_file_structure:
    - "main.py"
    - "search_service.py"
    - "scrape_service.py"
    - "synthesis_service.py"
    - "config.py"
    - "requirements.txt"
    - "output_report.md" # ç”Ÿæˆã•ã‚Œã‚‹æˆæžœç‰©

resources:
  domain_knowledge: |
    ã€é–‹ç™ºã®é‰„å‰‡ï¼ˆçµ¶å¯¾éµå®ˆï¼‰ã€‘
    1. **ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¹å‰²æ”¾æ£„ (Strict Rule):**
       - `main.py` ã¯ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆæŒ‡æ®è€…ï¼‰ã«å¾¹ã™ã‚‹ã“ã¨ã€‚
       - æ¤œç´¢ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã€è¦ç´„ãªã©ã®å…·ä½“çš„ãƒ­ã‚¸ãƒƒã‚¯ã‚’ `main.py` ã«æ›¸ãã“ã¨ã¯**åŽ³ç¦**ã¨ã™ã‚‹ã€‚å¿…ãšå„ `_service.py` ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦å‘¼ã³å‡ºã›ã€‚
    
    2. **ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®é˜²å¾¡åŠ›:**
       - Webã‚µã‚¤ãƒˆã¯ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦ï¼ˆ403ï¼‰ã‚„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒé »ç™ºã™ã‚‹ã€‚
       - `scrape_service.py` å†…ã§ã¯å¿…ãš `try-except` ã§ã‚¨ãƒ©ãƒ¼ã‚’æ¡ã‚Šæ½°ã—ã€**ã€Œ1ã¤ã®ã‚µã‚¤ãƒˆãŒãƒ€ãƒ¡ã§ã‚‚æ­¢ã¾ã‚‰ãšã«æ¬¡ã¸è¡Œãã€**æ§‹é€ ã«ã›ã‚ˆã€‚
       - å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã¯é•·ã™ãŽã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€å…ˆé ­ 10,000 æ–‡å­—ã§ãƒˆãƒªãƒŸãƒ³ã‚°ã™ã‚‹å‡¦ç†ã‚’å…¥ã‚Œã‚‹ã“ã¨ã€‚
    
    3. **æƒ…å ±ã®å‡ºå…¸ç®¡ç†:**
       - æ¤œç´¢çµæžœã® URL ã¨ ã‚¿ã‚¤ãƒˆãƒ« ã¯æœ€å¾Œã¾ã§ä¿æŒã—ã€æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®æœ«å°¾ã«ã€Œå‚è€ƒæ–‡çŒ®ãƒªã‚¹ãƒˆã€ã¨ã—ã¦è¨˜è¼‰ã™ã‚‹ã“ã¨ã€‚
    
    4. **æ®µéšŽçš„å‡¦ç†:**
       - ä¸€æ°—ã«ã‚„ã‚ã†ã¨ã—ãªã„ã“ã¨ã€‚
       - Step 1: æ¤œç´¢ã—ã¦URLãƒªã‚¹ãƒˆã‚’å¾—ã‚‹
       - Step 2: å„URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠœã
       - Step 3: ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã™ã‚‹
       - Step 4: è¦ç´„ã‚’çµ±åˆã—ã¦åŸ·ç­†ã™ã‚‹
       - ã“ã®ãƒ•ãƒ­ãƒ¼ã‚’ `main.py` ã§é †åºã‚ˆãå®Ÿè¡Œã›ã‚ˆã€‚


## File: `kits\chrome_extension_expert.yaml`

yaml
id: "chrome_extension_expert"
name: "Chrome Extension Expert (Manifest V3)"
description: "ãƒŸã‚¹ã‚’è¨±ã•ãªã„åŽ³æ ¼ãªChromeæ‹¡å¼µæ©Ÿèƒ½é–‹ç™ºã‚­ãƒƒãƒˆã€‚HTML/JSã®æ•´åˆæ€§ã‚’é‡è¦–ã€‚"
version: "2.0.0"

triggers:
  keywords:
    - "chrome extension"
    - "ã‚¯ãƒ­ãƒ¼ãƒ æ‹¡å¼µ"
    - "ãƒ—ãƒ©ã‚°ã‚¤ãƒ³"
    - "manifest v3"
  sample_prompts:
    - "Chromeæ‹¡å¼µæ©Ÿèƒ½ã‚’ä½œã£ã¦"
    - "ãƒ–ãƒ©ã‚¦ã‚¶ã®è¡¨ç¤ºã‚’å¤‰ãˆã‚‹æ‹¡å¼µæ©Ÿèƒ½"

blueprint:
  suggested_tech_stack:
    - "HTML5 / CSS3"
    - "JavaScript (ES6+)"
    - "Manifest V3"
  
  core_components:
    - "Popup UI"
    - "Background Service Worker"
    - "Content Scripts"
    - "Message Passing"

  # â˜…é‡è¦: ä½œæˆé †åºã‚’å¼·åˆ¶ã™ã‚‹ï¼ˆHTMLãŒå…ˆï¼ï¼‰
  expected_file_structure:
    - "manifest.json"
    - "popup/popup.html"  # å…ˆã«UIã‚’ä½œã‚‹
    - "popup/popup.js"    # æ¬¡ã«ãƒ­ã‚¸ãƒƒã‚¯
    - "popup/popup.css"
    - "background.js"
    - "icons/icon16.png" # (ãƒ€ãƒŸãƒ¼)

resources:
  domain_knowledge: |
    ã€é–‹ç™ºã®çµ¶å¯¾ãƒ«ãƒ¼ãƒ« (Strict Rules)ã€‘
    
    1. **é †åºã®åŽ³å®ˆ:**
       - å¿…ãš `popup.html` ã‚’ä½œæˆã—ã¦ã‹ã‚‰ã€ãã®å¾Œã« `popup.js` ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã€‚
       - AIã¯ã€Œå­˜åœ¨ã—ãªã„IDã€ã‚’æé€ ã—ãŒã¡ãªã®ã§ã€JSã‚’æ›¸ãéš›ã¯ç›´å‰ã«ä½œæˆã—ãŸHTMLã®IDã‚’ç¢ºèªã™ã‚‹ã“ã¨ã€‚

    2. **IDã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ (Anti-Hallucination):**
       - `popup.js` ã§ `document.getElementById('xyz')` ã‚’ä½¿ã†å ´åˆã€ãã®ID `'xyz'` ãŒ `popup.html` ã«å®Ÿåœ¨ã™ã‚‹ã“ã¨ã‚’100%ä¿è¨¼ã™ã‚‹ã“ã¨ã€‚
       - ã‚‚ã—HTMLã«ãªã„å ´åˆã¯ã€JSå´ã§å‹æ‰‹ã«å‚ç…§ã›ãšã€HTMLå´ã«IDã‚’è¿½åŠ ã™ã‚‹ä¿®æ­£æ¡ˆã‚’å‡ºã™ã“ã¨ã€‚
    
    2-b. **äº‹å‰ã‚¹ã‚­ãƒ£ãƒ³ã¨è‡ªå‹•IDè£œæ­£ (ID Auto-Sync):**
       - AIã¯ `popup.js` ã‚’ç”Ÿæˆã™ã‚‹å‰ã«å¿…ãš `popup.html` å†…ã®å…¨ã¦ã® `id` å±žæ€§ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€å‚ç…§å¯èƒ½ãªIDãƒªã‚¹ãƒˆã‚’å†…éƒ¨ã«ä¿æŒã™ã‚‹ã“ã¨ã€‚
       - JSå´ã§å­˜åœ¨ã—ãªã„IDã‚’å‚ç…§ã—ã‚ˆã†ã¨ã—ãŸå ´åˆã€å‹æ‰‹ã«JSã«æ›¸ãè¾¼ã‚€å‰ã«è‡ªå‹•ã§ `popup.html` ã«è©²å½“IDã‚’è¿½åŠ ã™ã‚‹ä¿®æ­£æ¡ˆã‚’ç”Ÿæˆã—ã¦ã‹ã‚‰JSã‚’ä½œæˆã™ã‚‹ã“ã¨ã€‚
       - ã“ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã†ã“ã¨ã§ã€HTMLã¨JSé–“ã®æ•´åˆæ€§ã‚’100%è‡ªå‹•ä¿è¨¼ã™ã‚‹ã€‚

    3. **Manifest V3ã®ç½ å›žé¿:**
       - `background` ã¯ `scripts` ã§ã¯ãªã `"service_worker"` ã‚’ä½¿ã†ã“ã¨ã€‚
       - `browser_action` ã§ã¯ãªã `"action"` ã‚’ä½¿ã†ã“ã¨ã€‚
       - `Content Security Policy (CSP)` ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã€ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆHTMLå†…ã® `<script>...code...</script>`ï¼‰ã¯ç¦æ­¢ã€‚å¿…ãšå¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ« (`popup.js`) ã«åˆ†é›¢ã™ã‚‹ã“ã¨ã€‚

    4. **éžåŒæœŸé€šä¿¡ã®é‰„å‰‡:**
       - `chrome.runtime.onMessage` ãƒªã‚¹ãƒŠãƒ¼å†…ã§éžåŒæœŸå‡¦ç†ï¼ˆ`sendResponse`ï¼‰ã‚’è¡Œã†å ´åˆã¯ã€å¿…ãš `return true;` ã‚’è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚ã“ã‚Œã‚’å¿˜ã‚Œã‚‹ã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒ£ãƒãƒ«ãŒå³åº§ã«é–‰ã˜ã‚‰ã‚Œã‚‹ã€‚


## File: `kits\data_analysis_dashboard.yaml`

yaml
id: "data_analysis_dashboard"
name: "Data Analysis & Visualization Dashboard"
description: "CSV/Excelãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€è‡ªå‹•ã§é›†è¨ˆãƒ»å¯è¦–åŒ–ã‚’è¡Œã†åˆ†æžãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"
version: "1.0.0"

triggers:
  keywords:
    - "ãƒ‡ãƒ¼ã‚¿"
    - "åˆ†æž"
    - "ã‚°ãƒ©ãƒ•"
    - "å¯è¦–åŒ–"
    - "csv"
    - "dashboard"
    - "plot"
  sample_prompts:
    - "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã‚°ãƒ©ãƒ•ã«ã™ã‚‹ã‚¢ãƒ—ãƒªã‚’ä½œã£ã¦"
    - "å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æžã™ã‚‹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒæ¬²ã—ã„"
    - "ãƒ‡ãƒ¼ã‚¿ã®å‚¾å‘ã‚’å¯è¦–åŒ–ã™ã‚‹ãƒ„ãƒ¼ãƒ«"

blueprint:
  suggested_tech_stack:
    - "Python 3.10+ (Flask)"
    - "Pandas (Data Processing)"
    - "Plotly / Matplotlib (Visualization)"
    - "Tailwind CSS (UI)"
    - "Chart.js / Plotly.js (Frontend Rendering)"

  core_components:
    - "File Uploader (CSV/Excel)"
    - "Data Processor (Pandas DataFrame)"
    - "Statistical Analyzer (Mean, Median, Corr)"
    - "Chart Generator API"

  expected_file_structure:
    - "app.py"
    - "analysis_logic.py"
    - "templates/index.html"
    - "requirements.txt"
    - "static/js/dashboard.js"

resources:
  domain_knowledge: |
    ã€ãƒ‡ãƒ¼ã‚¿åˆ†æžã‚¢ãƒ—ãƒªé–‹ç™ºã®é‰„å‰‡ã€‘
    1. **ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨Webè¡¨ç¤ºã®åˆ†é›¢:**
       - é‡ã„ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¯ `analysis_logic.py` å†…ã§è¡Œã„ã€`Pandas` ã‚’ãƒ•ãƒ«æ´»ç”¨ã™ã‚‹ã“ã¨ã€‚
       - `app.py` ã¯ãƒ‡ãƒ¼ã‚¿ã®å—ã‘æ¸¡ã—ã¨APIæä¾›ã«å¾¹ã™ã‚‹ã“ã¨ã€‚
    
    2. **ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†:**
       - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ©Ÿèƒ½ã‚’å¿…ãšå®Ÿè£…ã™ã‚‹ã“ã¨ã€‚
       - ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ `pd.read_csv()` ã§èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦æ‰±ã†ã€‚
       - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æ™‚ã¯ `werkzeug.utils.secure_filename` ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ« (`tempfile`) ã§å‡¦ç†ã™ã‚‹ã“ã¨ã€‚

    3. **å¯è¦–åŒ–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:**
       - ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã‚°ãƒ©ãƒ•ç”»åƒã‚’ç”Ÿæˆã—ã¦Base64ã§è¿”ã™æ–¹æ³•ï¼ˆMatplotlibï¼‰ã‹ã€ãƒ‡ãƒ¼ã‚¿ã‚’JSONã§è¿”ã—ã¦ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§æç”»ã™ã‚‹æ–¹æ³•ï¼ˆChart.js/Plotly.jsï¼‰ã®ã©ã¡ã‚‰ã‹ã‚’é¸æŠžã™ã‚‹ã€‚
       - **æŽ¨å¥¨:** ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚’JSONã§è¿”ã—ã€ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰(`Chart.js` ã¾ãŸã¯ `Plotly.js`)ã§æç”»ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆã™ã‚‹ã“ã¨ã€‚

    4. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°:**
       - èª­ã¿è¾¼ã‚ãªã„ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã‚„ã€ç©ºã®ãƒ‡ãƒ¼ã‚¿ãŒé€ã‚‰ã‚ŒãŸå ´åˆã®ä¾‹å¤–å‡¦ç† (`try-except`) ã‚’å¿…ãšå…¥ã‚Œã‚‹ã“ã¨ã€‚
       - æ•°å€¤ãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®åˆ—ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®å‡¦ç†ï¼ˆé™¤å¤–ã™ã‚‹ã‹ã€ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã‹ï¼‰ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã€‚


## File: `kits\git_profiler.yaml`

yaml
id: "git_repo_profiler"
name: "Git Repository AI Profiler"
description: "Gitãƒªãƒã‚¸ãƒˆãƒªã‚’è§£æžã—ã€ã‚³ãƒŸãƒƒãƒˆå±¥æ­´ã‚„ã‚³ãƒ¼ãƒ‰å¤‰æ›´é‡ã‹ã‚‰é–‹ç™ºã‚¹ã‚¿ã‚¤ãƒ«ã€è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚³ãƒ¼ãƒ‰ã®å¥å…¨æ€§ã‚’å¯è¦–åŒ–ï¼†AIè¨ºæ–­ã™ã‚‹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã€‚"
version: "1.0.0"

triggers:
  keywords: ["Git", "åˆ†æž", "å¯è¦–åŒ–", "ãƒªãƒã‚¸ãƒˆãƒª", "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«", "PyDriller"]
  sample_prompts:
    - "ã“ã®GitHubãƒªãƒã‚¸ãƒˆãƒªã®æ´»å‹•å±¥æ­´ã‚’åˆ†æžã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã«ã—ã¦"
    - "é–‹ç™ºãƒãƒ¼ãƒ ã®ã‚³ãƒ¼ãƒ‰ã‚³ãƒŸãƒƒãƒˆå‚¾å‘ã‚’å¯è¦–åŒ–ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’ä½œã£ã¦"

blueprint:
  suggested_tech_stack:
    - "Python 3.10+"
    - "PyDriller (Git Mining)"
    - "Pandas (Data Analysis)"
    - "Plotly (Interactive Charts)"
    - "Streamlit (Dashboard UI)"
    - "google-generativeai (Gemini 1.5 Flash)"

  core_components:
    - "Git Miner (Commit History Extractor)"
    - "Data Analyzer (Statistics & Metrics)"
    - "AI Profiler (Insights Generation)"
    - "Dashboard (Visualizations)"

  expected_file_structure:
    - "app.py"             # Streamlit Entry Point
    - "miner_service.py"   # PyDriller logic (Repo cloning & mining)
    - "analysis_service.py"# Pandas logic (Aggregations)
    - "ai_service.py"      # Gemini logic (Profile generation)
    - "charts.py"          # Plotly visualization logic
    - "requirements.txt"

resources:
  domain_knowledge: |
    ã€é–‹ç™ºã®é‰„å‰‡ã€‘
    1. **ãƒ‡ãƒ¼ã‚¿ãƒžã‚¤ãƒ‹ãƒ³ã‚°ã®åˆ†é›¢ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥:**
       - ãƒªãƒã‚¸ãƒˆãƒªè§£æžã¯é‡ã„ãŸã‚ã€`miner_service.py` ã§ `pydriller` ã‚’å®Ÿè¡Œã—ã€çµæžœï¼ˆã‚³ãƒŸãƒƒãƒˆæ—¥æ™‚ã€è‘—è€…ã€å¤‰æ›´è¡Œæ•°ã€ãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚
       - Streamlitã® `st.cache_data` ã‚’ä½¿ç”¨ã—ã¦ã€åŒã˜ãƒªãƒã‚¸ãƒˆãƒªã®å†è§£æžã‚’é˜²ãã“ã¨ã€‚
    
    2. **ã€Œæ´»å‹•ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ã€ã®å®Ÿè£…:**
       - GitHubã®è‰ï¼ˆContributionsï¼‰ã®ã‚ˆã†ãªã‚°ãƒ©ãƒ•ã ã‘ã§ãªãã€`Plotly` ã‚’ä½¿ã£ã¦ã€Œæ›œæ—¥ Ã— æ™‚é–“å¸¯ã€ã®ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ï¼ˆPunch Cardï¼‰ã‚’ä½œæˆã›ã‚ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Œæ·±å¤œç¨¼åƒçŽ‡ã€ãªã©ã‚’å¯è¦–åŒ–ã§ãã‚‹ã€‚
    
    3. **AIã«ã‚ˆã‚‹ã€Œé–‹ç™ºè€…æ€§æ ¼è¨ºæ–­ã€:**
       - é›†è¨ˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: å¹³å‡ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·ã€ä¿®æ­£é »åº¦ã€æ´»å‹•æ™‚é–“å¸¯ï¼‰ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã—ã¦Geminiã«æ¸¡ã—ã€ã€Œã“ã®é–‹ç™ºãƒãƒ¼ãƒ ã®å¼·ã¿ã¨å¥åº·çŠ¶æ…‹ã€ã‚’ãƒ¦ãƒ¼ãƒ¢ãƒ©ã‚¹ã‹ã¤é‹­ãåˆ†æžã•ã›ã‚‹ã“ã¨ã€‚
    
    4. **å®‰å…¨ãªã‚¯ãƒ­ãƒ¼ãƒ³:**
       - ãƒªãƒã‚¸ãƒˆãƒªã¯ Python ã® `tempfile` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã€åˆ†æžå¾Œã¯ç¢ºå®Ÿã«å‰Šé™¤ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰ã™ã‚‹è¨­è¨ˆã«ã™ã‚‹ã“ã¨ã€‚
    ã€åŽ³æ ¼ãªè²¬å‹™åˆ†é›¢ãƒ«ãƒ¼ãƒ«ã€‘
    
     app.py:
       - Streamlit UIã®ã¿ï¼ˆst.title, st.text_input, st.buttonãªã©ï¼‰
       - ä»–ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ä½¿ç”¨
       - ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ›¸ã‹ãªã„


## File: `kits\jra_keiba.yaml`

yaml
id: "jra_racing_prediction"
name: "JRA Horse Racing AI Predictor"
description: "JRA-VANãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œã—ãŸç«¶é¦¬äºˆæƒ³ãƒ»åˆ†æžã‚·ã‚¹ãƒ†ãƒ "
version: "2.1.0"

triggers:
  keywords: ["ç«¶é¦¬", "jra", "äºˆæƒ³", "é¦¬åˆ¸", "keiba", "å›žåŽçŽ‡"]
  sample_prompts:
    - "é€±æœ«ã®é‡è³žãƒ¬ãƒ¼ã‚¹ã‚’äºˆæƒ³ã™ã‚‹AIã‚’ä½œã£ã¦"
    - "è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æžã—ã¦ç©´é¦¬ã‚’è¦‹ã¤ã‘ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"

blueprint:
  suggested_tech_stack:
    - "Python 3.10+"
    - "pandas (ãƒ‡ãƒ¼ã‚¿åŠ å·¥)"
    - "scikit-learn / LightGBM (æ©Ÿæ¢°å­¦ç¿’)"
    - "BeautifulSoup (è£œåŠ©çš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°)"

  core_components:
    - "Data Preprocessing Pipeline (æ¬ æå€¤å‡¦ç†ãƒ»ã‚«ãƒ†ã‚´ãƒªå¤‰æ›)"
    - "Feature Engineering (è¡€çµ±ãƒ»é¨Žæ‰‹ãƒ»éŽåŽ»èµ°ç ´ã‚¿ã‚¤ãƒ )"
    - "Backtesting Engine (å›žåŽçŽ‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)"

  expected_file_structure:
    - "main.py"
    - "data_loader.py"
    - "model.py"
    - "strategies/bloodline.py"

resources:
  domain_knowledge: |
    ã€é‡è¦ã€‘æ—¥æœ¬ã®ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿åˆ†æžã«ãŠã‘ã‚‹é‰„å‰‡:
    1. ã€Œé¦¬å ´çŠ¶æ…‹ï¼ˆè‰¯ãƒ»ç¨é‡ãƒ»é‡ãƒ»ä¸è‰¯ï¼‰ã€ã¯æœ€é‡è¦ç‰¹å¾´é‡ã®ä¸€ã¤ã€‚å¿…ãšè€ƒæ…®ã™ã‚‹ã“ã¨ã€‚
    2. è¡€çµ±ãƒ‡ãƒ¼ã‚¿ï¼ˆçˆ¶ãƒ»æ¯çˆ¶ï¼‰ã¯ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¨ã—ã¦æ‰±ã†ã‚ˆã‚Šã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒæœ‰åŠ¹ã€‚
    3. ã‚¿ã‚¤ãƒ æŒ‡æ•°ï¼ˆSpeed Indexï¼‰ã‚’è¨ˆç®—ã™ã‚‹éš›ã¯ã€ç«¶é¦¬å ´ã”ã¨ã®åŸºæº–ã‚¿ã‚¤ãƒ å·®ã‚’è£œæ­£ã™ã‚‹ã“ã¨ã€‚
    4. 3é€£å˜ã®äºˆæ¸¬ã¯ãƒŽã‚¤ã‚ºãŒå¤šã„ãŸã‚ã€ã¾ãšã¯ã€Œè¤‡å‹åœå†…ï¼ˆ3ç€ä»¥å†…ï¼‰ã€ã®ç¢ºçŽ‡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æŽ¨å¥¨ã€‚


## File: `kits\mahjong_pro.yaml`

yaml
id: "mahjong_browser_game"
name: "Mahjong Browser Game (Full Stack)"
description: "Pythonã®åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã¨ã€JSã®ã‚²ãƒ¼ãƒ é€²è¡Œã‚’çµ„ã¿åˆã‚ã›ãŸã€å®Ÿéš›ã«éŠã¹ã‚‹ä¸€äººéº»é›€"
version: "3.0.0"

triggers:
  keywords: ["éº»é›€", "mahjong", "ã‚²ãƒ¼ãƒ ", "ãƒ—ãƒ¬ã‚¤"]
  sample_prompts:
    - "ãƒ–ãƒ©ã‚¦ã‚¶ã§éŠã¹ã‚‹éº»é›€ã‚²ãƒ¼ãƒ ã‚’ä½œã£ã¦"
    - "ä¸€äººéº»é›€ã‚¢ãƒ—ãƒª"

blueprint:
  suggested_tech_stack:
    - "Python 3.10 (Flask Backend)"
    - "Vanilla JavaScript (Frontend Logic)"
    - "Tailwind CSS (UI Design)"
    - "mahjong (Library: https://pypi.org/project/mahjong/)"

  core_components:
    - "Game Loop (Init -> Draw -> Discard -> Check -> Repeat)"
    - "Shanten Calculation API"
    - "Visual Tile Rendering (CSS/Unicode)"

  expected_file_structure:
    - "app.py"
    - "templates/index.html"  # ã“ã‚ŒãŒãªã„ã¨å§‹ã¾ã‚‰ãªã„
    - "requirements.txt"

resources:
  domain_knowledge: |
    ã€é–‹ç™ºã®çµ¶å¯¾ãƒ«ãƒ¼ãƒ«ã€‘
    1. **ã€Œè¨ˆç®—æ©Ÿã€ã§ã¯ãªãã€Œã‚²ãƒ¼ãƒ ã€ã‚’ä½œã‚‹ã“ã¨ã€‚**
       - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰‹å…¥åŠ›ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒãƒ©ãƒ³ãƒ€ãƒ ã«é…ç‰Œã™ã‚‹ã“ã¨ã€‚
       - ã€Œãƒ„ãƒ¢ãƒœã‚¿ãƒ³ã€ã¨ã€Œã‚¯ãƒªãƒƒã‚¯ã§æ¨ã¦ç‰Œã€ã®æ©Ÿèƒ½ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã€‚
    
    2. **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ­£ã—ã„ä½¿ã„æ–¹ (Copy this!)**
       - `mahjong` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚·ãƒ£ãƒ³ãƒ†ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’åŽ³å®ˆã™ã‚‹ã“ã¨ã€‚
       (å¤ã„ `mahjong.hand` ã¯å­˜åœ¨ã—ãªã„ãŸã‚ä½¿ã‚ãªã„ã“ã¨)
       
       ```python
       from mahjong.shanten import Shanten
       from mahjong.tile import TilesConverter
       
       # 13æžšã¾ãŸã¯14æžšã®æ‰‹ç‰Œãƒªã‚¹ãƒˆ (ä¾‹: ['1m', '2m'...]) ã‚’å—ã‘å–ã‚‹
       tiles_34 = TilesConverter.to_34_array(tiles_list)
       calculator = Shanten()
       result = calculator.calculate_shanten(tiles_34)
       # result: -1=Agari, 0=Tenpai, 1=1-Shanten...
       ```

    3. **ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®æŒ™å‹•**
       - `index.html` å†…ã« `<script>` ã‚¿ã‚°ã§ã‚²ãƒ¼ãƒ ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå±±ç‰Œç®¡ç†ã€æ‰‹ç‰Œé…åˆ—ï¼‰ã‚’æ›¸ãã“ã¨ã€‚
       - ç‰Œã‚’æ¨ã¦ã‚‹ãŸã³ã« `fetch('/api/check', ...)` ã§ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«å•ã„åˆã‚ã›ã¦ã€ã‚·ãƒ£ãƒ³ãƒ†ãƒ³æ•°ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ã€‚


## File: `kits\ml_model_api_wrapper.yaml`

yaml
id: "ml_model_api_wrapper"
name: "Machine Learning Model API Wrapper"
description: "æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹: scikit-learn, PyTorchï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€REST APIã¨ã—ã¦å…¬é–‹ã™ã‚‹APIã‚­ãƒƒãƒˆã€‚"
version: "1.0.0"

triggers:
  keywords:
    - "æ©Ÿæ¢°å­¦ç¿’"
    - "AIãƒ¢ãƒ‡ãƒ«"
    - "äºˆæ¸¬"
    - "APIå…¬é–‹"
    - "scikit-learn"
  sample_prompts:
    - "å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’APIã¨ã—ã¦å…¬é–‹ã—ãŸã„"
    - "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‹ã‚‰äºˆæ¸¬çµæžœã‚’è¿”ã™Webã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œã£ã¦"

blueprint:
  suggested_tech_stack:
    - "Python 3.10+ (FastAPI/Flask)"
    - "Scikit-learn / TensorFlow (Model Library)"
    - "Numpy (ãƒ‡ãƒ¼ã‚¿å‡¦ç†)"
    - "FastAPI (APIæä¾›ã€‚Flaskã‚ˆã‚Šé«˜é€Ÿã§ãƒ¢ãƒ€ãƒ³ãªãŸã‚æŽ¨å¥¨)"

  core_components:
    - "Model Loader (pickle, joblib)"
    - "Data Validator (å…¥åŠ›ã®åž‹ãƒã‚§ãƒƒã‚¯)"
    - "Prediction Endpoint (/predict)"

  expected_file_structure:
    - "main.py"
    - "model_wrapper.py"
    - "model.pkl" # (ãƒ€ãƒŸãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦è¨˜è¼‰)
    - "requirements.txt"

resources:
  domain_knowledge: |
    ã€MLãƒ¢ãƒ‡ãƒ«APIã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€‘
    1. **Model Loading:** ãƒ¢ãƒ‡ãƒ«ã¯ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—å¤–ï¼ˆèµ·å‹•æ™‚ï¼‰ã«ä¸€åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ¡ãƒ¢ãƒªã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã“ã¨ï¼ˆæŽ¨è«–é€Ÿåº¦ã®ãŸã‚ï¼‰ã€‚
    2. **Input Validation:** APIã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã¯å¿…ãšNumpyé…åˆ—ã«å¤‰æ›ã—ã€å½¢çŠ¶ï¼ˆshapeï¼‰ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã“ã¨ã€‚
    3. **FastAPIã®ä½¿ç”¨:** äºˆæ¸¬ã‚µãƒ¼ãƒ“ã‚¹ã¯ä½Žé…å»¶ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ãŸã‚ã€FastAPIã®éžåŒæœŸå‡¦ç†ã‚’å„ªå…ˆã™ã‚‹ã“ã¨ã€‚
    4. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£:** ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.pklãªã©ï¼‰ã¯ç›´æŽ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã›ãšã€ã‚³ãƒ¼ãƒ‰ã§å‚ç…§ã•ã›ã‚‹ã“ã¨ã€‚


## File: `kits\smart_bartender.yaml`

yaml
id: "smart_bartender_ai"
name: "Smart Bartender & Cabinet Manager"
description: "è‡ªå®…ã«ã‚ã‚‹ãŠé…’ã‚„ã‚¸ãƒ¥ãƒ¼ã‚¹ï¼ˆåœ¨åº«ï¼‰ã‚’ç™»éŒ²ã—ã€ãã‚Œã‚‰ã§ä½œã‚Œã‚‹ã‚«ã‚¯ãƒ†ãƒ«ã‚’GeminiãŒææ¡ˆãƒ»å‰µä½œã™ã‚‹ã‚¢ãƒ—ãƒªã€‚SQLiteã§åœ¨åº«ç®¡ç†ã‚’è¡Œã†ã€‚"
version: "1.0.0"

triggers:
  keywords: ["ã‚«ã‚¯ãƒ†ãƒ«", "ãƒãƒ¼ãƒ†ãƒ³ãƒ€ãƒ¼", "ãŠé…’", "ãƒ¬ã‚·ãƒ”ç”Ÿæˆ", "åœ¨åº«ç®¡ç†"]
  sample_prompts:
    - "ä»Šã‚ã‚‹ãŠé…’ã§ä½œã‚Œã‚‹ã‚«ã‚¯ãƒ†ãƒ«ã‚’æ•™ãˆã¦"
    - "ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼ã¨ç‚­é…¸æ°´ã‚’ä½¿ã£ãŸã‚¢ãƒ¬ãƒ³ã‚¸ãƒ¬ã‚·ãƒ”ã‚’è€ƒãˆã¦"

blueprint:
  suggested_tech_stack:
    - "Python 3.10+"
    - "Streamlit (Web UI)"
    - "google-generativeai (Gemini 2.5 Flash)"
    - "SQLAlchemy (Inventory DB)"
    - "Pydantic (Recipe Schema Validation)"

  core_components:
    - "Inventory Manager (CRUD for Liquors/Mixers)"
    - "Bartender Brain (LLM Recipe Generator)"
    - "UI Controller"
    - "Database Model"

  expected_file_structure:
    - "app.py"             # UIã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    - "inventory_service.py" # åœ¨åº«ç®¡ç†ãƒ­ã‚¸ãƒƒã‚¯
    - "bartender_service.py" # Geminiã¨ã®å¯¾è©±ãƒ­ã‚¸ãƒƒã‚¯
    - "models.py"          # DBã¨Pydanticã®ãƒ¢ãƒ‡ãƒ«å®šç¾©
    - "database.py"        # DBæŽ¥ç¶šå‘¨ã‚Š
    - "requirements.txt"

resources:
  domain_knowledge: |
    ã€é–‹ç™ºã®é‰„å‰‡ã€‘
    1. **ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ åŒ– (Strict JSON Output):**
       - AIãŒç”Ÿæˆã™ã‚‹ãƒ¬ã‚·ãƒ”ã¯ã€å¿…ãšJSONå½¢å¼ã§å—ã‘å–ã‚Šã€Pydanticãƒ¢ãƒ‡ãƒ«ã§ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ã“ã¨ã€‚
       - å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: `name` (åå‰), `ingredients` (ææ–™ãƒªã‚¹ãƒˆ), `instructions` (æ‰‹é †), `flavor_profile` (å‘³ã®ç‰¹å¾´), `alcohol_strength` (åº¦æ•°ç›®å®‰)ã€‚
       - `bartender_service.py` å†…ã§ `response_schema` ã‚’ä½¿ç”¨ã—ã¦Geminiã«å¼·åˆ¶ã™ã‚‹ã“ã¨ã€‚
    
    2. **åœ¨åº«ãƒ™ãƒ¼ã‚¹ã®æ€è€ƒ:**
       - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã¯å¿…ãšã€Œç¾åœ¨ã®åœ¨åº«ãƒªã‚¹ãƒˆï¼ˆInventoryï¼‰ã€ã‚’å«ã‚ã€ã€Œã“ã‚Œã«å«ã¾ã‚Œã‚‹ææ–™ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã›ã‚ˆã€ã¨æŒ‡ç¤ºã™ã‚‹ã“ã¨ã€‚
       - è¶³ã‚Šãªã„ææ–™ãŒã‚ã‚‹å ´åˆã¯ã€ã€Œã‚ã¨ã“ã‚ŒãŒã‚ã‚Œã°ä½œã‚Œã¾ã™ã€ã¨ã„ã†ææ¡ˆï¼ˆMissing Ingredientsï¼‰ã‚’å«ã‚ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã«ã™ã‚‹ã¨è¦ªåˆ‡ã€‚
    
    3. **UI/UX:**
       - Streamlitã® `st.data_editor` ã‚’ä½¿ã£ã¦ã€åœ¨åº«ã®è¿½åŠ ãƒ»å‰Šé™¤ã‚’ç›´æ„Ÿçš„ã«è¡Œãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã€‚
       - ç”Ÿæˆã•ã‚ŒãŸã‚«ã‚¯ãƒ†ãƒ«ã¯ã‚«ãƒ¼ãƒ‰å½¢å¼ã§è¦‹ã‚„ã™ãè¡¨ç¤ºã™ã‚‹ã“ã¨ã€‚


## File: `kits\social_bot_kit.yaml`

yaml
id: "social_bot_automation"
name: "Social Media Auto-Poster (Selenium)"
description: "Google Sheetsã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€Seleniumã‚’ä½¿ã£ã¦X/Threadsã«è‡ªå‹•æŠ•ç¨¿ã™ã‚‹Bot"
version: "1.0.0"

triggers:
  keywords: ["è‡ªå‹•æŠ•ç¨¿", "SNS", "bot", "selenium", "x", "threads"]
  sample_prompts:
    - "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®å†…å®¹ã‚’Xã¨Threadsã«æŠ•ç¨¿ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’ä½œã£ã¦"
    - "SNSè‡ªå‹•æŠ•ç¨¿ãƒ„ãƒ¼ãƒ«"

blueprint:
  suggested_tech_stack:
    - "Python 3.10"
    - "Selenium (Browser Automation)"
    - "gspread (Google Sheets API)"
    - "schedule (Task Scheduling)"
    - "oauth2client (Auth)"

  core_components:
    - "Sheets Loader (Read time & content)"
    - "Browser Manager (Headless Chrome)"
    - "X Poster Logic (XPath selector)"
    - "Threads Poster Logic"
    - "Scheduler Loop"

  expected_file_structure:
    - "main.py"
    - "poster_logic.py"
    - "sheets_handler.py"
    - ".env"
    - "requirements.txt"

resources:
  domain_knowledge: |
    ã€é–‹ç™ºã®é‰„å‰‡ã€‘
    1. **APIã§ã¯ãªãSeleniumã‚’ä½¿ã†:**
       - Xã¨Threadsã¯APIåˆ¶é™ãŒåŽ³ã—ã„ãŸã‚ã€`selenium` ã¨ `webdriver_manager` ã‚’ä½¿ç”¨ã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œã§æŠ•ç¨¿ã‚’è¡Œã†ã“ã¨ã€‚
       - ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã¯å¿…ãš `.env` ã‹ã‚‰èª­ã¿è¾¼ã‚€ã“ã¨ã€‚
    
    2. **è¦ç´ ã®ç‰¹å®š (XPath):**
       - Xã®æŠ•ç¨¿ãƒœã‚¿ãƒ³ã‚„ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã¯ `data-testid` å±žæ€§ã‚’ä½¿ã£ã¦ç‰¹å®šã™ã‚‹ã®ãŒæœ€ã‚‚å®‰å®šã™ã‚‹ã€‚
       - ä¾‹: ãƒ„ã‚¤ãƒ¼ãƒˆãƒœã‚¿ãƒ³ -> `//div[@data-testid='tweetButtonInline']`
    
    3. **Google Sheetsé€£æº:**
       - `gspread` ã‚’ä½¿ç”¨ã—ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã‚­ãƒ¼ã‚’ä½¿ã£ã¦èªè¨¼ã™ã‚‹æ§‹é€ ã«ã™ã‚‹ã“ã¨ã€‚
    
    4. **å¾…æ©Ÿå‡¦ç†:**
       - ãƒšãƒ¼ã‚¸é·ç§»ã‚„æŠ•ç¨¿å®Œäº†ã‚’å¾…ã¤ãŸã‚ã« `time.sleep` ã§ã¯ãªã `WebDriverWait` ã‚’ä½¿ã†ã“ã¨ã€‚


## File: `kits\web_article_repurposer.yaml`

yaml
id: "web_article_repurposer"
name: "Web Article to Blog Engine"
description: "Webè¨˜äº‹ã®URLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºã—ã€Gemini 2.5 Flashã§è¦ç´„ãƒ»å†æ§‹æˆã—ã¦ã€ãƒ–ãƒ­ã‚°è¨˜äº‹ã¨SNSæŠ•ç¨¿æ–‡ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹Streamlitã‚¢ãƒ—ãƒªã€‚"
version: "1.0.0"

triggers:
  keywords: ["Webè¦ç´„", "è¨˜äº‹è¦ç´„", "ãƒ–ãƒ­ã‚°è‡ªå‹•ç”Ÿæˆ", "ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°", "Streamlit"]
  sample_prompts:
    - "ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ãƒ–ãƒ­ã‚°ã«æ›¸ãç›´ã—ã¦"
    - "URLã‹ã‚‰è¦ç´„ã¨ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ä½œã‚‹ã‚¢ãƒ—ãƒª"

blueprint:
  suggested_tech_stack:
    - "Python 3.10+"
    - "requests (HTTP Client)"
    - "beautifulsoup4 (HTML Parser)"
    - "google-generativeai (Gemini 1.5 Flash)"
    - "Streamlit (Web UI)"

  core_components:
    - "Scrape Service (Fetch & Parse)"
    - "AI Service (Summarize & Rewrite)"
    - "UI Controller (Streamlit View)"
    - "Main Entry Point"

  expected_file_structure:
    - "app.py"             # UIã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    - "scrape_service.py"  # è¨˜äº‹æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
    - "ai_service.py"      # Gemini APIã¨ã®é€šä¿¡ãƒ­ã‚¸ãƒƒã‚¯
    - "utils.py"           # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    - "requirements.txt"

resources:
  domain_knowledge: |
    ã€é–‹ç™ºã®é‰„å‰‡ï¼ˆçµ¶å¯¾åŽ³å®ˆï¼‰ã€‘
    1. **ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¹å‰²åˆ¶é™ (Strict Rule):**
       - `app.py` ã«ã¯ **UIã®è¡¨ç¤ºã‚³ãƒ¼ãƒ‰ï¼ˆãƒœã‚¿ãƒ³ã€ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ãªã©ï¼‰ä»¥å¤–ã‚’æ›¸ã„ã¦ã¯ãªã‚‰ãªã„**ã€‚
       - ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚„AIç”Ÿæˆã®å®Ÿè£…ã¯ã€å¿…ãš `scrape_service.py` ã‚„ `ai_service.py` ã«è¨˜è¿°ã—ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚
       - **`main.py` (app.py) ã¸ã®ãƒ­ã‚¸ãƒƒã‚¯ãƒ™ã‚¿æ›¸ãã¯åŽ³ç¦ã€‚**
    
    2. **ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®å®‰å®šæ€§:**
       - ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆãªã©ã¯ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ãŒåŽ³ã—ã„å ´åˆãŒã‚ã‚‹ãŸã‚ã€`User-Agent` ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é©åˆ‡ã«è¨­å®šã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã™ã‚‹ã“ã¨ã€‚
       - æœ¬æ–‡æŠ½å‡ºãŒé›£ã—ã„å ´åˆã§ã‚‚ã‚¨ãƒ©ãƒ¼ã§è½ã¡ãšã€ã€Œæœ¬æ–‡ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€ã¨UIã«è¡¨ç¤ºã™ã‚‹å®‰å…¨è¨­è¨ˆã«ã™ã‚‹ã“ã¨ã€‚
    
    3. **AIãƒ¢ãƒ‡ãƒ«:**
       - é«˜é€Ÿã‹ã¤å®‰ä¾¡ãª **Gemini 2.5 Flash** ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚


## File: `kits\web_flask.yaml`

yaml
id: "web_flask_standard"
name: "Standard Flask Web App"
description: "Python Flaskã‚’ä½¿ç”¨ã—ãŸå …ç‰¢ãªWebã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ§‹æˆ"
version: "1.0.0"

triggers:
  keywords: ["web", "flask", "site", "homepage", "ã‚¢ãƒ—ãƒª"]
  sample_prompts:
    - "Flaskã§ToDoã‚¢ãƒ—ãƒªã‚’ä½œã£ã¦"
    - "ã‚·ãƒ³ãƒ—ãƒ«ãªWebã‚µã‚¤ãƒˆã‚’æ§‹ç¯‰ã—ãŸã„"

blueprint:
  suggested_tech_stack:
    - "Python 3.10+"
    - "Flask (Web Framework)"
    - "SQLite (Database)"
    - "Bootstrap 5 (CSS Framework)"
  
  core_components:
    - "Application Factory Pattern (create_app)"
    - "Blueprints (ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°åˆ†å‰²)"
    - "Jinja2 Templates"

  expected_file_structure:
    - "app.py"
    - "requirements.txt"
    - "src/__init__.py"
    - "templates/base.html"

resources:
  domain_knowledge: |
    Flaskã‚¢ãƒ—ãƒªã§ã¯ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’ä½¿ã‚ãš 'Application Factory' ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŽ¡ç”¨ã—ã¦ãã ã•ã„ã€‚
    HTMLã¯å¿…ãš 'templates/base.html' ã‚’ç¶™æ‰¿ã—ã€é‡è¤‡ã‚³ãƒ¼ãƒ‰ã‚’é˜²ãã“ã¨ã€‚
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«ç¢ºå®Ÿã«ã‚¯ãƒ­ãƒ¼ã‚ºã™ã‚‹ã“ã¨ã€‚


## File: `src\__init__.py`

py



## File: `src\config.py`

py
import os
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()

@dataclass(frozen=True)
class Settings:
    # --- API Configuration ---
    LLM_API_KEY: str = os.getenv("GEMINI_API_KEY", "")
    
    # --- Model Strategy (å…¨ãƒ¢ãƒ‡ãƒ«ã‚’Standard Flashã«çµ±ä¸€ã—ã€åˆæœŸå“è³ªã‚’æœ€å¤§åŒ–) ---
    # Flash-Liteã¯å»ƒæ­¢ã—ã€ç”Ÿæˆã‹ã‚‰ä¿®å¾©ã¾ã§å…¨ã¦Standard Flashã§å®Ÿè¡Œ
    LLM_MODEL_FAST: str = "gemini-2.5-flash"
    LLM_MODEL_HEALER: str = "gemini-2.5-flash"
    LLM_MODEL_SMART: str = "gemini-2.5-flash"
    LLM_MODEL_AUDIT: str = "gemini-2.5-flash"
    
    # --- Budget ---
    MAX_BUDGET_PER_RUN: float = 50.0  # 1å›žã®å®Ÿè¡Œä¸Šé™ (å††)
    
    # --- Application Paths ---
    BASE_DIR: str = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    OUTPUT_DIR: str = os.path.join(BASE_DIR, "evo_output")
    KITS_DIR: str = os.path.join(BASE_DIR, "kits")
    
    # --- Runtime Settings ---
    DOCKER_IMAGE: str = "evo-sandbox"
    CONTAINER_PREFIX: str = "evo-dev"
    MAX_RETRIES: int = 1 # 1å›žå‹è² ã«å›ºå®š

    def __post_init__(self):
        os.makedirs(self.OUTPUT_DIR, exist_ok=True)
        os.makedirs(self.KITS_DIR, exist_ok=True)

config = Settings()


## File: `src\services\architect_service.py`

py
import json
import re
import logging
from typing import List, Dict, Optional, Tuple

logger = logging.getLogger("Architect")

class ArchitectService:
    
    def __init__(self, client, kit_manager):
        self.client = client
        self.kit_manager = kit_manager

    # æˆ»ã‚Šå€¤ã®åž‹ãƒ’ãƒ³ãƒˆã‚’Tuple[List[Dict], Optional[Dict]]ã¨ã™ã‚‹
    def create_plan(self, user_prompt: str) -> Tuple[List[Dict], Optional[Dict]]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã¨Kitã«åŸºã¥ã„ã¦å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚ºè¨ˆç”»ã‚’ä½œæˆã—ã€ãã®è¨ˆç”»ã¨ä½¿ç”¨ã—ãŸKitã‚’è¿”ã™ã€‚
        """
        
        # 1. ã‚­ãƒƒãƒˆã®é¸æŠžãƒ­ã‚¸ãƒƒã‚¯ã‚’çµ±åˆ
        matches = self.kit_manager.find_best_match(user_prompt)
        kit = matches[0][0] if matches else None
        
        kit_info = ""
        if kit:
            logger.info(f"ðŸ§© Kit Auto-Selected: {kit.get('name')}")
            # ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„: å¿…è¦ãªæƒ…å ±ã ã‘ã‚’æŠ½å‡º
            tech_stack = ', '.join(kit.get('blueprint', {}).get('suggested_tech_stack', []))
            core_comps = ', '.join(kit.get('blueprint', {}).get('core_components', []))
            kit_info = f"## Active Kit: {kit.get('name')}\nStack: {tech_stack}\nComponents: {core_comps}\n"

        sys_prompt = f"""
        You are a Software Architect.
        Break down the user's request into logical implementation phases.
        
        {kit_info}
        
        # CRITICAL RULES:
        1. Separation: UI Logic -> app.py (Streamlit allowed). Data Logic -> *_service.py (NO Streamlit).
        2. Simplicity: Create minimum viable files.
        3. Output JSON Array only.
        """
        
        try:
            # Planã®ç”Ÿæˆ (Flash Standardãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)
            response = self.client.generate(f"Request: {user_prompt}", sys_prompt)
            
            json_str = response.strip()

            # ðŸš¨ ä¿®æ­£: æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã‚’å¼•ãèµ·ã“ã—ã¦ã„ãŸæ­£è¦è¡¨ç¾ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã€æ–‡å­—åˆ—æ“ä½œã«ç½®ãæ›ãˆ ðŸš¨
            start_index = json_str.find('[')
            last_index = json_str.rfind(']')
            
            if start_index == -1 or last_index == -1 or start_index > last_index:
                 # JSONé…åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
                 raise ValueError("JSON array boundary ([...]) not found in response.")

            # []ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’æŠ½å‡º
            final_json_data = json_str[start_index : last_index + 1]

            if final_json_data:
                parsed_plan = json.loads(final_json_data)
                
                # â˜…â˜…â˜… ä¿®æ­£ãƒ­ã‚¸ãƒƒã‚¯ã®è¿½åŠ  â˜…â˜…â˜…
                # Planã®ã€Œfilesã€ãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã€è¾žæ›¸ã§ã¯ãªããƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ–‡å­—åˆ—ï¼‰ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹
                cleaned_plan = self._clean_plan_files(parsed_plan)
                
                # Planã¨Kitã‚’ä¸¡æ–¹è¿”ã™ (2è¦ç´ ã®ã‚¿ãƒ—ãƒ«)
                return (cleaned_plan, kit) 
            else:
                raise ValueError("Extracted JSON data is empty.")
                
        except Exception as e:
            logger.error(f"Planning failed: {e}. Attempting fallback.")
            # ðŸ’¡ ä¿®æ­£: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ©ãƒ³ã‚’ç©ºã®ãƒªã‚¹ãƒˆã§ã¯ãªãã€app.pyã‚’ç”Ÿæˆã™ã‚‹ãƒ—ãƒ©ãƒ³ã«æˆ»ã™
            # ã“ã®ãƒ—ãƒ©ãƒ³ãŒå£Šã‚Œã¦ã„ãªã„ã‹ã€å¿µã®ãŸã‚æ§‹é€ ãƒã‚§ãƒƒã‚¯ã‚’å¼·åˆ¶ã™ã‚‹
            fallback_plan = [{"phase": "1", "description": "Implementation", "files": ["app.py"]}]
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ©ãƒ³è‡ªä½“ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’ã‹ã‘ã‚‹ (äºŒé‡ä¿è¨¼)
            cleaned_fallback = self._clean_plan_files(fallback_plan)

            return (cleaned_fallback, kit) 
            
    def _clean_plan_files(self, plan: List[Dict]) -> List[Dict]:
        """
        LLMãŒã€Œfilesã€ãƒªã‚¹ãƒˆå†…ã«è¾žæ›¸ã‚’å…¥ã‚Œã¦ã—ã¾ã£ãŸå ´åˆã€ãã‚Œã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«å¤‰æ›ã™ã‚‹ã€‚
        ã¾ãŸã€ãƒ—ãƒ©ãƒ³è¦ç´ ãŒå£Šã‚Œã¦ã„ãªã„ã‹ç¢ºèªã™ã‚‹ã€‚
        """
        cleaned_plan = []
        for step in plan:

            # --- SUPER PATCH 2: èª¬æ˜Žæ–‡ã®æºã‚‰ãŽå¸åŽ ---
            for k in ['objective', 'summary', 'desc', 'overview', 'goal']:
                if k in step: step['description'] = step.pop(k)
            # ----------------------------------------


            # --- SUPER PATCH: ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ ---
            # 1. æ—¢çŸ¥ã®ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            known_keys = ['target_files', 'files_to_modify', 'file_list', 'modified_files', 'files_to_create', 'code_files', 'output_files']
            for k in known_keys:
                if k in step: step['files'] = step.pop(k)

            # 2. ãã‚Œã§ã‚‚ç„¡ã‘ã‚Œã°ã€å€¤ã®ä¸­èº«ã‚’èµ°æŸ»ã—ã¦ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã£ã½ã„ãƒªã‚¹ãƒˆã€ã‚’è‡ªå‹•ç™ºè¦‹ã™ã‚‹
            if 'files' not in step:
                for v in step.values():
                    # æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã§ã€.py ã‚„ .txt ã§çµ‚ã‚ã‚‹ã‚‚ã®ãŒã‚ã‚Œã°æŽ¡ç”¨
                    if isinstance(v, list) and v and isinstance(v[0], str) and (v[0].endswith('.py') or v[0].endswith('.txt')):
                        step['files'] = v
                        break
                    # è¾žæ›¸ã®ãƒªã‚¹ãƒˆã§ã€filenameã‚­ãƒ¼ã‚’æŒã£ã¦ã„ãŸã‚‰æŽ¡ç”¨
                    if isinstance(v, list) and v and isinstance(v[0], dict) and ('filename' in v[0] or 'name' in v[0]):
                        step['files'] = v
                        break
            # ----------------------------------------


            # --- æŸ”è»Ÿæ€§å‘ä¸Šãƒ‘ãƒƒãƒ: ã‚ã‚‰ã‚†ã‚‹ã‚­ãƒ¼ã®æºã‚‰ãŽã‚’å¸åŽ ---
            for k in ['phase_title', 'phase_name', 'name', 'step_name', 'title']:
                if k in step: step['phase'] = step.pop(k)
            
            for k in ['target_files', 'files_to_modify', 'file_list', 'modified_files']:
                if k in step: step['files'] = step.pop(k)
            # ------------------------------------------------

            # Planã®è¦ç´ ãŒè¾žæ›¸ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            if not isinstance(step, dict):
                logger.warning(f"Skipping malformed plan step: {step}")
                continue
                
            files = step.get('files', [])
            cleaned_files = []
            
            for f in files:
                if isinstance(f, dict):
                    # è¾žæ›¸ã®å ´åˆã¯'filename'ã‚­ãƒ¼ã‚’æŽ¢ã—ã¦æ–‡å­—åˆ—ã«å¤‰æ›
                    if 'filename' in f:
                        cleaned_files.append(f['filename'])
                    elif 'name' in f:
                        cleaned_files.append(f['name'])
                elif isinstance(f, str):
                    # æ–‡å­—åˆ—ã¯ãã®ã¾ã¾æŽ¡ç”¨
                    cleaned_files.append(f)

            # å¿µã®ãŸã‚ã€'phase'ã‚­ãƒ¼ãŒæ¬ è½ã—ã¦ã„ã‚‹å ´åˆã«å‚™ãˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ©ãƒ³ã§ä¿è¨¼ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            for k in ['phase_title', 'phase_name', 'name', 'step_name']:
                if k in step: step['phase'] = step.pop(k)
            if 'phase' not in step or 'description' not in step:
                 logger.warning(f"Plan step missing required keys: {step}")
                 # å£Šã‚ŒãŸã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
                 continue

            step['files'] = cleaned_files
            cleaned_plan.append(step)
            
        return cleaned_plan


## File: `src\services\budget_service.py`

py
import logging

logger = logging.getLogger("BudgetGuard")

class BudgetGuard:
    def __init__(self, limit_yen=50.0):
        self.limit_yen = limit_yen
        self.current_cost = 0.0
        
        # 100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®å˜ä¾¡ç›®å®‰ (å††) - $1=150å††æ›ç®—
        # å‚è€ƒ: https://ai.google.dev/gemini-api/docs/pricing
        
        # Flash / Flash-Lite: 
        # Input $0.075 (~11.25å††) / Output $0.30 (~45å††) 
        # â€»ä»¥å‰ã®ç”»åƒ($0.10/$0.40)ã‚ˆã‚Šå®‰ããªã£ã¦ã„ã¾ã™ãŒã€å®‰å…¨å´ã«å€’ã—ã¦å°‘ã—é«˜ã‚ã«è¨­å®šã™ã‚‹ã‹ã€æ­£ç¢ºã«åˆã‚ã›ã‚‹ã‹ã€‚
        # ã“ã“ã§ã¯å®‰å…¨ãƒžãƒ¼ã‚¸ãƒ³è¾¼ã¿ã§ $0.10 / $0.40 (15å†† / 60å††) ã‚’ç¶­æŒã—ã¤ã¤ã€Proã‚’ä¿®æ­£ã—ã¾ã™ã€‚
        
        # Pro: 
        # Input $1.25 (~187.5å††) / Output $10.00 (~1500å††)
        # â€»<=128kã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã€‚ã“ã‚Œã‚’è¶…ãˆã‚‹ã¨å€é¡ã«ãªã‚Šã¾ã™ãŒã€åŸºæœ¬ã¯ã“ã¡ã‚‰ã‚’ä½¿ç”¨ã€‚
        
        self.rates = {
            # Flashç³» (Liteå«ã‚€)
            "gemini-2.5-flash-lite": {"input": 15.0,  "output": 60.0},
            "gemini-2.0-flash":      {"input": 15.0,  "output": 60.0},
            "gemini-2.5-flash":      {"input": 15.0,  "output": 60.0}, 
            "gemini-1.5-flash":      {"input": 15.0,  "output": 60.0},
            
            # Pro / High-Intelligenceç³» (1.5, 2.0, 2.5, 3.0)
            "gemini-2.5-pro":        {"input": 187.5, "output": 1500.0},
            "gemini-2.0-pro":        {"input": 187.5, "output": 1500.0},
            "gemini-1.5-pro":        {"input": 187.5, "output": 1500.0},
            "gemini-3":              {"input": 300.0, "output": 1800.0}, # Gemini 3ã¯ä»®ã®é«˜ã‚è¨­å®š
        }

    def check_and_record(self, model_name: str, input_chars: int, output_chars: int):
        """ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—ã—ã€ç´¯ç©ã™ã‚‹ã€‚ä¸Šé™ã‚’è¶…ãˆãŸã‚‰ä¾‹å¤–ã‚’æŠ•ã’ã‚‹ã€‚"""
        rate = None
        # éƒ¨åˆ†ä¸€è‡´ã§ãƒ¬ãƒ¼ãƒˆã‚’æŽ¢ã™ (ä¾‹: "models/gemini-1.5-pro-latest" -> "gemini-1.5-pro")
        for key in self.rates:
            if key in model_name:
                rate = self.rates[key]
                break
        
        if not rate:
            # å®‰å…¨ç­–: "pro" ãŒåå‰ã«å«ã¾ã‚Œã¦ã„ãŸã‚‰Proä¾¡æ ¼ã€ãã‚Œä»¥å¤–ã¯Flashä¾¡æ ¼ã‚’é©ç”¨
            if "pro" in model_name.lower():
                rate = self.rates["gemini-1.5-pro"]
            else:
                rate = self.rates["gemini-2.5-flash-lite"]

        input_cost = (input_chars / 1_000_000) * rate["input"]
        output_cost = (output_chars / 1_000_000) * rate["output"]
        total_cost = input_cost + output_cost
        
        self.current_cost += total_cost
        
        logger.info(f"ðŸ’° Cost: +{total_cost:.4f}å†† (Total: {self.current_cost:.2f} / {self.limit_yen}å††) [{model_name}]")

        if self.current_cost > self.limit_yen:
            logger.error("ðŸ’¸ BUDGET EXCEEDED! Stopping execution to save money.")
            raise Exception(f"Budget Limit Exceeded: Used {self.current_cost:.2f}JPY (Limit: {self.limit_yen}JPY)")


## File: `src\services\data_recorder.py`

py
import json
import os
import logging
from datetime import datetime
from src.config import config

logger = logging.getLogger("DataRecorder")

class DataRecorder:
    def __init__(self):
        self.data_dir = os.path.join(config.BASE_DIR, "datasets")
        os.makedirs(self.data_dir, exist_ok=True)
        self.dataset_path = os.path.join(self.data_dir, "evo_success_log.jsonl")

    def save_success(self, prompt: str, kit_name: str, final_files: dict):
        """
        æˆåŠŸä½“é¨“ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è¿½åŠ ã™ã‚‹
        Format: Alpaca / Llama 3 Instruction Tuning Format
        """
        try:
            # å¿…è¦ãªã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã‚’æŠ½å‡º
            code_content = ""
            for fname, content in final_files.items():
                if fname.endswith(('.py', '.js', '.html', '.css')):
                    code_content += f"# File: {fname}\n{content}\n\n"

            entry = {
                "timestamp": datetime.now().isoformat(),
                "instruction": prompt,
                "input": f"Use Kit: {kit_name}" if kit_name else "No Kit",
                "output": code_content,
                "system": "You are Evo, an expert AI developer."
            }

            # JSONLå½¢å¼ï¼ˆ1è¡Œ1JSONï¼‰ã§è¿½è¨˜
            with open(self.dataset_path, "a", encoding="utf-8") as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
            
            logger.info(f"ðŸ’¾ Success data recorded to {self.dataset_path}")

        except Exception as e:
            logger.error(f"Failed to record data: {e}")


## File: `src\services\git_service.py`

py
import subprocess
import os
import logging
from src.config import config

logger = logging.getLogger("Git")

class GitService:
    def __init__(self):
        self.cwd = config.OUTPUT_DIR
        self._init_repo()

    def _init_repo(self):
        if not os.path.exists(os.path.join(self.cwd, ".git")):
            try:
                self._run(["init"])
                # .gitignoreä½œæˆ
                with open(os.path.join(self.cwd, ".gitignore"), "w") as f:
                    f.write(".venv/\n__pycache__/\n_trash/\n*.log\n")
                self._run(["add", "."])
                self._run(["commit", "-m", "Init"])
            except: pass

    def commit(self, msg):
        try:
            self._run(["add", "."])
            # å¤‰æ›´ãŒã‚ã‚‹å ´åˆã®ã¿ã‚³ãƒŸãƒƒãƒˆ
            if self._run(["status", "--porcelain"], capture_output=True):
                self._run(["commit", "-m", msg])
                logger.info(f"ðŸ•°ï¸ Git saved: {msg}")
        except: pass

    # â˜…è¿½åŠ : ç¾åœ¨ã®ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥ã‚’å–å¾— (ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨)
    def get_head_hash(self):
        try: return self._run(["rev-parse", "HEAD"], capture_output=True).strip()
        except: return None

    # â˜…è¿½åŠ : æŒ‡å®šã—ãŸã‚³ãƒŸãƒƒãƒˆã¾ã§å¼·åˆ¶çš„ã«å·»ãæˆ»ã™
    def revert_to(self, commit_hash):
        if not commit_hash: return
        try:
            self._run(["reset", "--hard", commit_hash])
            logger.warning(f"âª Reverted code to snapshot: {commit_hash[:7]}")
        except Exception as e:
            logger.error(f"Revert failed: {e}")

    def _run(self, args, capture_output=False):
        return subprocess.run(
            ["git"] + args, 
            cwd=self.cwd, 
            check=True, 
            capture_output=capture_output, 
            text=True, 
            encoding='utf-8'
        ).stdout


## File: `src\services\healer_service.py`

py
import logging
import hashlib
from typing import Dict, List, Tuple, Optional
from src.services.patch_service import PatchService

logger = logging.getLogger("Healer")

class HealerService:
    def __init__(self, fast_client, healer_client):
        self.fast = fast_client     # L1/L2
        self.healer = healer_client # L3 (Flash Standard)
        self.patcher = PatchService()
        self.repair_history = {} 

    def build_context(self, files: Dict[str, str]) -> str:
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºå‰Šæ¸›: å…ˆé ­1000æ–‡å­—ã ã‘æ¸¡ã™
        context = []
        for name, content in files.items():
            snippet = content[:1000] + "\n...(truncated)..." if len(content) > 1000 else content
            context.append(f"File: {name}\n```\n{snippet}\n```")
        return "\n".join(context)

    def heal(self, fname: str, content: str, errors: List[str], context_files: Dict, kit: Optional[Dict] = None) -> Tuple[bool, str, str]:
        error_msg = errors[0] if errors else "Unknown error"
        
        # --- ãƒ«ãƒ¼ãƒ—æ¤œçŸ¥ãƒ­ã‚¸ãƒƒã‚¯ ---
        error_hash = hashlib.md5(error_msg.encode('utf-8')).hexdigest()
        history_key = f"{fname}:{error_hash}"
        current_tries = self.repair_history.get(history_key, 0)
        
        if current_tries >= 2: # 2å›žè©¦ã—ã¦ãƒ€ãƒ¡ãªã‚‰è«¦ã‚ã‚‹
            logger.warning(f"ðŸ›‘ Healing Loop Detected for {fname}. Ignoring error and proceeding.")
            # â˜…é‡è¦: Falseã§ã¯ãªãTrueã‚’è¿”ã—ã€å¤‰æ›´ãªã—ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿”ã™ã“ã¨ã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ­¢ã‚ãªã„
            return True, content, "Loop_Ignored"
        
        self.repair_history[history_key] = current_tries + 1

        context_str = self.build_context(context_files)
        kit_instruction = ""
        if kit:
            kit_instruction = f"Context: {kit.get('name')}"

        base_prompt = f"""
        Fix code in '{fname}'.
        Error: {error_msg}
        {kit_instruction}
        
        Current Code:
        {content}
        
        Reference:
        {context_str}
        """

        # L2: Patch (å®‰ä¾¡)
        try:
            prompt_l2 = base_prompt + "\nReturn a SEARCH/REPLACE block (<<<< SEARCH ... ==== ... >>>>)."
            patch_res = self.fast.generate(prompt_l2)
            patched_code = self.patcher.apply_patch(content, patch_res)
            if patched_code: return True, patched_code, "L2_Patch"
        except Exception: pass

        # L3: Rewrite (é«˜ä¾¡ã ãŒç¢ºå®Ÿ) - ãƒ«ãƒ¼ãƒ—1å›žç›®ã®æ™‚ã ã‘è©¦ã™
        if current_tries == 0:
            try:
                prompt_l3 = base_prompt + "\nRewrite the FULL file correctly. Output only the code."
                fixed_res = self.healer.generate(prompt_l3)
                fixed_code = self._clean_code(fixed_res)
                if len(fixed_code) > 10: return True, fixed_code, "L3_Rewrite"
            except Exception as e:
                logger.error(f"Healer failed: {e}")

        # ä¿®æ­£ã§ããªãã¦ã‚‚ã€ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã•ã›ãªã„ãŸã‚ã«å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™
        logger.warning(f"âš ï¸ Could not fix {fname}. Keeping original.")
        return True, content, "Skipped"

    def _clean_code(self, text):
        return text.replace("```python", "").replace("```", "").strip()


## File: `src\services\kit_gen_service.py`

py
import logging
from typing import Dict, Optional

logger = logging.getLogger("KitGenService")

class KitGenService:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã«åŸºã¥ã„ã¦ã€Evoè‡ªèº«ã®æ‹¡å¼µãƒ—ãƒ©ã‚°ã‚¤ãƒ³(Kit YAML)ã‚’ç”Ÿæˆã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚
    è‡ªå·±é€²åŒ–ã®ä¸­æ ¸ã‚’æ‹…ã†ã€‚
    """
    def __init__(self, client):
        self.client = client # Smart Client (Pro/Flash) ã‚’ä½¿ç”¨

    def generate_kit(self, user_prompt: str) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶è¨€èªžè¨˜è¿°ã‹ã‚‰ã€æœ‰åŠ¹ãªKit YAMLã‚’ç”Ÿæˆã™ã‚‹
        """
        system_prompt = """
        ã‚ãªãŸã¯AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ŒEvoã€ã®æ©Ÿèƒ½æ‹¡å¼µã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã«åŸºã¥ãã€EvoãŒç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚’é‚è¡Œã™ã‚‹ãŸã‚ã®ã€ŒKitï¼ˆå°‚é–€çŸ¥è­˜å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã€ã‚’YAMLå½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

        ã€Kitã®æ§‹æˆè¦ç´ ã€‘
        1. id: ä¸€æ„ã®è­˜åˆ¥å­ (è‹±æ•°å­—ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢)
        2. name: ã‚ã‹ã‚Šã‚„ã™ã„åå‰
        3. description: ä½•ã‚’ã™ã‚‹Kitã‹
        4. triggers: ã“ã®KitãŒç™ºå‹•ã™ã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        5. blueprint: æŽ¨å¥¨æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã€ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ
        6. resources: **æœ€é‡è¦**ã€‚AIã«ä¸Žãˆã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã€è¨­è¨ˆæ€æƒ³ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€‚

        ã€å‡ºåŠ›ãƒ«ãƒ¼ãƒ«ã€‘
        - å¿…ãšæœ‰åŠ¹ãªYAMLå½¢å¼ã®ã¿ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚
        - ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ (```yaml ... ```) ã§å›²ã‚€ã“ã¨ã€‚
        - `domain_knowledge` ã¯å…·ä½“çš„ã‹ã¤å°‚é–€çš„ã«æ›¸ãã“ã¨ï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ­£ã—ã„ä½¿ã„æ–¹ã€è½ã¨ã—ç©´ã€è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãªã©ï¼‰ã€‚

        ã€å‡ºåŠ›ä¾‹ã€‘
        ```yaml
        id: "discord_bot_py"
        name: "Discord Bot Builder"
        description: "discord.pyã‚’ä½¿ç”¨ã—ãŸé«˜æ©Ÿèƒ½Boté–‹ç™ºã‚­ãƒƒãƒˆ"
        triggers:
          keywords: ["discord", "bot", "ãƒ‡ã‚£ã‚¹ã‚³ãƒ¼ãƒ‰"]
          sample_prompts: ["ã‚µãƒ¼ãƒãƒ¼ç®¡ç†Botã‚’ä½œã£ã¦"]
        blueprint:
          suggested_tech_stack: ["Python 3.10", "discord.py", "python-dotenv"]
          core_components: ["Event Listener", "Command Tree", "Cog System"]
          expected_file_structure:
            - "main.py"
            - "cogs/general.py"
            - ".env"
        resources:
          domain_knowledge: |
            discord.py 2.0ä»¥é™ã§ã¯ `Intents` ã®è¨­å®šãŒå¿…é ˆã§ã™ã€‚
            å¤§è¦æ¨¡ãªBotã®å ´åˆã¯ `Cogs` æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã‚³ãƒžãƒ³ãƒ‰ã‚’åˆ†å‰²ç®¡ç†ã—ã¦ãã ã•ã„ã€‚
            ãƒˆãƒ¼ã‚¯ãƒ³ã¯å¿…ãšç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ã“ã¨ã€‚
        ```
        """

        prompt = f"ä»¥ä¸‹ã®è¦æœ›ã‚’æº€ãŸã™Kitã‚’ä½œæˆã—ã¦ãã ã•ã„:\n{user_prompt}"
        
        logger.info("ðŸ§  Generating new Kit definition...")
        response = self.client.generate(prompt, system_prompt)
        
        # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° (Markdowné™¤åŽ»)
        yaml_content = response.replace("```yaml", "").replace("```", "").strip()
        return yaml_content


## File: `src\services\kit_manager.py`

py
import os
import yaml
import glob
import logging
from typing import Dict, List, Tuple
from src.config import config

logger = logging.getLogger("KitManager")

class KitManager:
    def __init__(self, client=None):
        self.kits = {}
        # Clientã¯å—ã‘å–ã‚‹ãŒã€åŸºæœ¬ä½¿ã‚ãªã„ï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
        self.client = client
        self._load_all_kits()
    
    def _load_all_kits(self):
        self.kits = {}
        if not os.path.exists(config.KITS_DIR): return
        
        for f in glob.glob(os.path.join(config.KITS_DIR, "*.yaml")):
            try:
                with open(f, 'r', encoding='utf-8') as fp:
                    data = yaml.safe_load(fp)
                    if data and 'id' in data:
                        self.kits[data['id']] = data
            except Exception as e:
                logger.error(f"Error loading kit {f}: {e}")
        
        logger.info(f"ðŸ“¦ Kits Loaded: {len(self.kits)}")

    def find_best_match(self, prompt: str, top_n=1) -> List[Tuple[Dict, float]]:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒžãƒƒãƒã®ã¿ã‚’ä½¿ç”¨ã—ã€LLMã‚³ã‚¹ãƒˆã‚’ã‚¼ãƒ­ã«ã™ã‚‹ã€‚
        """
        matches = []
        p_lower = prompt.lower()
        
        for kit_id, kit in self.kits.items():
            score = 0
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´
            for kw in kit.get('triggers', {}).get('keywords', []):
                if kw.lower() in p_lower: 
                    score += 5.0 # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ’ãƒƒãƒˆã¯é‡ã¿ã‚’å¤§ãã
            
            # èª¬æ˜Žæ–‡ã®éƒ¨åˆ†ä¸€è‡´ï¼ˆç°¡æ˜“çš„ï¼‰
            if kit.get('description', '').lower() in p_lower:
                score += 2.0

            if score > 0:
                matches.append((kit, score))
        
        matches.sort(key=lambda x: x[1], reverse=True)
        
        if matches:
            logger.info(f"âš¡ Kit matched by keyword: {matches[0][0]['name']}")
            return matches[:top_n]
        
        # ãƒžãƒƒãƒã—ãªã‹ã£ãŸå ´åˆã€AIã«èžããƒ­ã‚¸ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹ã“ã¨ã‚‚ã§ãã‚‹ãŒã€
        # ã‚³ã‚¹ãƒˆå„ªå…ˆãªã‚‰ã€Œã‚­ãƒƒãƒˆãªã—ã€ã§é€²ã‚ã‚‹ã®ãŒæ­£è§£ã€‚
        return []

    def save_new_kit(self, yaml_content: str) -> str:
        try:
            data = yaml.safe_load(yaml_content)
            if not data or 'id' not in data: raise ValueError("Invalid YAML")
            path = os.path.join(config.KITS_DIR, f"{data['id']}.yaml")
            with open(path, 'w', encoding='utf-8') as f:
                f.write(yaml_content)
            self._load_all_kits()
            return data.get('name', data['id'])
        except Exception as e:
            logger.error(f"Failed to save kit: {e}")
            raise e


## File: `src\services\patch_service.py`

py
import re
import logging
from typing import Optional

logger = logging.getLogger("EvoPatchService")

class PatchService:
    @staticmethod
    def normalize(code: str) -> str:
        code = re.sub(r'#.*', '', code)
        return re.sub(r'\s+', ' ', code).strip()

    def apply_patch(self, original_code: str, patch_text: str) -> Optional[str]:
        pattern = re.compile(r"<<<< SEARCH\n(.*?)\n====\n(.*?)\n>>>>", re.DOTALL)
        matches = pattern.findall(patch_text)
        if not matches: return None
        
        new_code = original_code
        
        for search_block, replace_block in matches:
            if original_code.count(search_block) > 1:
                logger.warning("Patch Skipped: Non-unique search block.")
                return None
            if search_block in new_code:
                new_code = new_code.replace(search_block, replace_block, 1)
                continue
            
            # Fuzzy match fallback
            search_norm = self.normalize(search_block)
            lines = new_code.split('\n')
            n_search = len(search_block.split('\n'))
            
            for i in range(len(lines) - n_search + 1):
                candidate_block = "\n".join(lines[i:i+n_search])
                if self.normalize(candidate_block) == search_norm:
                    lines[i:i+n_search] = replace_block.split('\n')
                    new_code = "\n".join(lines)
                    break
        return new_code if new_code != original_code else None


## File: `src\services\qa_service.py`

py
import logging

logger = logging.getLogger("EvoQA")

class QualityAssuranceService:
    def __init__(self, client):
        self.client = client # Healerã¨åŒã˜è³¢ã„ãƒ¢ãƒ‡ãƒ«(Flash)æŽ¨å¥¨

    def audit_and_fix(self, project_files: dict) -> str:
        """
        æœ€çµ‚ç›£æŸ»: ãƒ•ã‚¡ã‚¤ãƒ«é–“ã®ä¸æ•´åˆï¼ˆImportãƒŸã‚¹ã€é–¢æ•°å¼•æ•°ä¸ä¸€è‡´ï¼‰ã®ã¿ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã€‚
        ãƒ­ã‚¸ãƒƒã‚¯ã®ä¸­èº«ã¾ã§ã¯è¦‹ãªã„ã“ã¨ã§ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã€‚
        """
        context_str = self._build_lightweight_context(project_files)
        if not context_str: return ""
        
        system_prompt = """
        Role: QA Engineer.
        Task: Check consistency between files. Ignore logic bugs inside functions.
        
        Focus on:
        1. **Import Errors**: Does the imported function exist in the target file?
        2. **Signature Mismatch**: Do function calls match definitions?
        3. **HTML/JS IDs**: Do JS `getElementById` IDs match HTML `id`s?

        Output Format:
        If you find a CRITICAL integration bug, output the FULL fixed file content:
        # FILENAME: path/to/file.py
        ```python
        ... code ...
        ```
        If everything looks consistent, output NOTHING.
        """
        
        user_prompt = f"Audit these file interfaces:\n\n{context_str}"

        try:
            # è³¢ã„ãƒ¢ãƒ‡ãƒ«ã§ä¸€ç™ºã§æ±ºã‚ã‚‹
            return self.client.generate(user_prompt, system_prompt)
        except Exception as e:
            logger.error(f"Audit failed: {e}")
            return ""

    def _build_lightweight_context(self, project_files):
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºå‰Šæ¸›: 
        # ã‚³ãƒ¼ãƒ‰ã®ä¸­èº«ã‚’å…¨éƒ¨æ¸¡ã™ã®ã§ã¯ãªãã€æ§‹é€ ã‚’æ¸¡ã™ã¹ãã ãŒã€
        # ä¿®æ­£ã•ã›ã‚‹ãŸã‚ã«ã¯ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦ã€‚
        # å¦¥å”æ¡ˆ: ä¸»è¦ãªã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿æ¸¡ã—ã€å·¨å¤§ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚„Configã¯é™¤å¤–ã™ã‚‹ã€‚
        
        valid_exts = {'.py', '.js', '.html'}
        content = []
        
        for fname, code in project_files.items():
            if any(fname.endswith(ext) for ext in valid_exts):
                # 2000è¡Œã‚’è¶…ãˆã‚‹ã‚ˆã†ãªå·¨å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€å…ˆé ­ã¨æœ«å°¾ã ã‘æ¸¡ã™ç­‰ã®å·¥å¤«ã‚‚å¯èƒ½ã ãŒã€
                # ã“ã“ã§ã¯å˜ç´”ã«æ–‡å­—æ•°åˆ¶é™ã‚’è¨­ã‘ã‚‹
                if len(code) > 20000: 
                    snippet = code[:5000] + "\n... (truncated for QA) ...\n" + code[-5000:]
                else:
                    snippet = code
                content.append(f"# FILENAME: {fname}\n```\n{snippet}\n```")
        
        return "\n".join(content)


## File: `src\services\search_service.py`

py
import logging
from ddgs import DDGS # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åå¤‰æ›´ã«å¯¾å¿œ
from src.config import config

logger = logging.getLogger("SearchService")

class SearchService:
    """
    Webæ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ (Cost Optimized)
    ãƒšãƒ¼ã‚¸ã”ã¨ã®è¦ç´„(Nå›ž)ã‚’ã‚„ã‚ã€ã‚¹ãƒ‹ãƒšãƒƒãƒˆé›†ç´„â†’æœ€çµ‚å›žç­”(1å›ž)ã«å¤‰æ›´ã€‚
    """
    def __init__(self, client):
        self.client = client # Flash-Lite
        self.ddgs = DDGS()

    def research(self, query: str, max_results=3) -> str:
        logger.info(f"ðŸ” Searching for: '{query}'...")
        
        try:
            # 1. DuckDuckGoã§æ¤œç´¢ (ç„¡æ–™)
            # bodyã‚­ãƒ¼ã«ã‚¹ãƒ‹ãƒšãƒƒãƒˆãŒå…¥ã£ã¦ã„ã‚‹ã®ã§ã“ã‚Œã‚’ä½¿ã†
            results = list(self.ddgs.text(query, max_results=max_results))
            if not results:
                return "No search results found."
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return f"Search Error: {str(e)}"

        # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é›†ç´„ (ãƒšãƒ¼ã‚¸ã«ã¯ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„)
        # å®Ÿéš›ã«ãƒšãƒ¼ã‚¸ã‚’é–‹ã„ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã®ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã€
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾ç­–ã§å¤±æ•—ã™ã‚‹ã“ã¨ã‚‚å¤šã„ãŸã‚ã€æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®è¦ç´„ã‚’ä¿¡ã˜ã‚‹ã€‚
        
        context_data = ""
        for i, r in enumerate(results):
            title = r.get('title', 'No Title')
            link = r.get('href', '')
            snippet = r.get('body', '')
            context_data += f"Source {i+1}: {title}\nURL: {link}\nSummary: {snippet}\n\n"

        # 3. 1å›žã ã‘LLMã‚’å‘¼ã³å‡ºã—ã¦ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        prompt = f"""
        User Query: "{query}"

        Search Results:
        {context_data}

        Task: Summarize the search results to answer the user's query.
        Focus on technical details (libraries, code usage, installation).
        Output Format: Markdown
        """
        
        try:
            report = self.client.generate(prompt, "Role: Tech Researcher. Output: Concise technical summary.")
            return report
        except Exception as e:
            return f"Failed to generate report: {e}"


## File: `src\services\structure_service.py`

py
import os
import ast
import json
import logging
from typing import Dict, List

logger = logging.getLogger("StructureService")

class StructureService:
    def __init__(self):
        self.dependency_graph = {}
        self.symbol_table = {}

    def analyze_project(self, files: Dict[str, str]) -> str:
        """
        ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã‚’è§£æžã—ã€ä¾å­˜é–¢ä¿‚ã¨å®šç¾©æ¸ˆã¿ã‚·ãƒ³ãƒœãƒ«ï¼ˆé–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ï¼‰ã®ãƒžãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹ã€‚
        ã“ã‚Œã‚’LLMã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€å…¨ä½“æ§‹é€ ã‚’ç†è§£ã•ã›ã‚‹ã€‚
        """
        self.dependency_graph = {}
        self.symbol_table = {}

        for fname, content in files.items():
            if fname.endswith('.py'):
                self._analyze_python_file(fname, content)
        
        # LLMã«æ¸¡ã™ãŸã‚ã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        summary = "# Project Structure Summary\n"
        
        summary += "## Defined Symbols (Classes & Functions):\n"
        for fname, symbols in self.symbol_table.items():
            # ã‚·ãƒ³ãƒœãƒ«ãŒãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„
            if not symbols: continue
            
            summary += f"- **{fname}**:\n"
            for sym in symbols:
                summary += f"  - `{sym['type']} {sym['name']}` (Line {sym['line']})\n"
        
        summary += "\n## Dependencies (Imports):\n"
        for fname, deps in self.dependency_graph.items():
            if deps:
                summary += f"- **{fname}** depends on: {', '.join(deps)}\n"
                
        return summary

    def _analyze_python_file(self, fname: str, code: str):
        try:
            tree = ast.parse(code)
            symbols = []
            imports = []

            for node in ast.walk(tree):
                # ã‚¯ãƒ©ã‚¹å®šç¾©
                if isinstance(node, ast.ClassDef):
                    # ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„: ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¯ãƒ©ã‚¹ï¼ˆ_ã§å§‹ã¾ã‚‹ï¼‰ã¯åœ°å›³ã«è¼‰ã›ãªã„
                    if node.name.startswith('_'): continue 
                    symbols.append({'type': 'class', 'name': node.name, 'line': node.lineno})
                
                # é–¢æ•°å®šç¾©
                elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    # ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„: ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆé–¢æ•°ï¼ˆ_ã§å§‹ã¾ã‚‹ï¼‰ã¯åœ°å›³ã«è¼‰ã›ãªã„
                    if node.name.startswith('_'): continue
                    symbols.append({'type': 'function', 'name': node.name, 'line': node.lineno})
                
                # ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    if isinstance(node, ast.Import):
                        for n in node.names: imports.append(n.name.split('.')[0])
                    elif node.module:
                        imports.append(node.module.split('.')[0])

            self.symbol_table[fname] = symbols
            self.dependency_graph[fname] = list(set(imports)) # é‡è¤‡æŽ’é™¤

        except Exception as e:
            logger.warning(f"Failed to parse {fname}: {e}")


## File: `src\services\verifier_service.py`

py
import ast
import logging
import autopep8
import os
import re
import json
from bs4 import BeautifulSoup
from typing import Dict, List, Set

logger = logging.getLogger("Verifier")

class VerifierService:
    def __init__(self, runtime):
        self.runtime = runtime
        # å±é™ºãªæ“ä½œã®ã¿ç¦æ­¢ã€‚ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä½¿ç”¨åˆ¶é™ã¯æ’¤å»ƒï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ï¼‰
        self.BANNED_MODULES = ['subprocess', 'socket']
        self.BANNED_FUNCTIONS = ['eval', 'exec'] 

    def format_code(self, code: str, filename: str) -> str:
        try:
            if filename.endswith(".py"):
                return autopep8.fix_code(code, options={'aggressive': 1})
            if filename.endswith(('.html', '.js', '.css', '.json', '.yaml', '.md')):
                return code.strip() + "\n"
        except: pass
        return code

    def verify(self, code: str, filename: str, context_files: dict = None) -> dict:
        """
        é™çš„è§£æž: è‡´å‘½çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ã¿ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ (éŽå‰°å“è³ªã®æŽ’é™¤)
        """
        ext = os.path.splitext(filename)[1].lower()
        errors = []

        # 1. Pythonã®æ¤œæŸ»
        if ext == '.py':
            try: tree = ast.parse(code)
            except SyntaxError as e: return {"valid": False, "errors": [f"Python Syntax: {e}"]}
            
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã®ã¿å®Ÿæ–½
            sec = self._check_banned_nodes(tree)
            if not sec['valid']: errors.extend(sec['errors'])
            
            # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒã‚§ãƒƒã‚¯(_check_architecture)ã¯å»ƒæ­¢
            # ç†ç”±: AIãŒæ··ä¹±ã—ã€ä¿®æ­£ãƒ«ãƒ¼ãƒ—ã«é™¥ã‚‹æœ€å¤§ã®åŽŸå› ã§ã‚ã‚‹ãŸã‚ã€‚
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ (Importã‚¨ãƒ©ãƒ¼ã®ã¿ç¢ºèª)
            if context_files:
                symbol_table = self._build_symbol_table(context_files)
                import_errors = self._verify_imports(tree, filename, symbol_table)
                errors.extend(import_errors)

        # 2. JSON, HTMLæ¤œæŸ»
        elif ext == '.json':
            try: json.loads(code)
            except Exception as e: errors.append(f"JSON Error: {e}")
        elif ext == '.html':
            if '<body>' not in code and '<body ' not in code: errors.append("Missing <body> tag")

        return {"valid": len(errors) == 0, "errors": errors}

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ãªã©ã®éŽå‰°æ©Ÿèƒ½ã¯å‰Šé™¤ã—ã€å˜ç´”åŒ–
    
    def _build_symbol_table(self, files: Dict[str, str]) -> Dict[str, Set[str]]:
        symbols = {}
        for fname, content in files.items():
            if not fname.endswith('.py'): continue
            module_name = os.path.splitext(os.path.basename(fname))[0]
            defined = set()
            try:
                tree = ast.parse(content)
                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                        defined.add(node.name)
                symbols[module_name] = defined
                symbols[fname] = defined 
            except: pass
        return symbols

    def _verify_imports(self, tree: ast.AST, current_filename: str, symbol_table: Dict[str, Set[str]]) -> List[str]:
        errors = []
        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom) and node.module:
                module_name = node.module
                # å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã‚¹ã‚­ãƒƒãƒ—
                if module_name not in symbol_table and module_name + ".py" not in symbol_table:
                    continue
                
                defined_symbols = symbol_table.get(module_name, set())
                if not defined_symbols: defined_symbols = symbol_table.get(module_name + ".py", set())

                for alias in node.names:
                    if alias.name == '*': continue
                    if alias.name not in defined_symbols:
                        # è‡´å‘½çš„ã§ã¯ãªã„ãŒã€è­¦å‘Šã¨ã—ã¦è¨˜éŒ²
                        errors.append(f"Import Warning: '{alias.name}' not found in '{module_name}'.")
        return errors

    def _check_banned_nodes(self, tree):
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for a in node.names:
                    if a.name.split('.')[0] in self.BANNED_MODULES: return {"valid":False, "errors":[f"Banned import: {a.name}"]}
            elif isinstance(node, ast.ImportFrom) and node.module:
                if node.module.split('.')[0] in self.BANNED_MODULES: return {"valid":False, "errors":[f"Banned import: {node.module}"]}
            elif isinstance(node, ast.Call):
                func_name = None
                if isinstance(node.func, ast.Name): func_name = node.func.id
                elif isinstance(node.func, ast.Attribute) and isinstance(node.func.value, ast.Name):
                    if node.func.value.id in self.BANNED_MODULES: return {"valid":False, "errors":[f"Banned call: {node.func.value.id}"]}
                if func_name in self.BANNED_FUNCTIONS: return {"valid":False, "errors":[f"Banned function: {func_name}"]}
        return {"valid": True, "errors": []}


## File: `src\services\workspace_manager.py`

py
import os
import re
import logging
from typing import Dict
from src.config import config
from src.services.git_service import GitService

logger = logging.getLogger("Workspace")

class WorkspaceManager:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã€Gitã€ã‚³ãƒ¼ãƒ‰ã®ãƒ‘ãƒ¼ã‚¹ãªã©ã€
    ã€Œæ€è€ƒã€ä»¥å¤–ã®ã€Œä½œæ¥­ã€ã‚’ä¸€æ‰‹ã«å¼•ãå—ã‘ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    def __init__(self):
        self.project_files: Dict[str, str] = {}
        self.git = GitService()
        self._load_workspace()

    def _load_workspace(self):
        self.project_files = {}
        ignore = {'.venv', '__pycache__', '_trash', '.git', 'node_modules'}
        for root, dirs, files in os.walk(config.OUTPUT_DIR):
            dirs[:] = [d for d in dirs if d not in ignore]
            for file in files:
                if file.endswith(('.py', '.html', '.js', '.css', '.json', '.md', '.txt', '.yaml')):
                    try:
                        path = os.path.join(root, file)
                        rel_path = os.path.relpath(path, config.OUTPUT_DIR).replace("\\", "/")
                        with open(path, 'r', encoding='utf-8') as f:
                            self.project_files[rel_path] = f.read()
                    except: pass

    def save_file(self, fname: str, content: str):
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®é™¤åŽ»ãªã©ã‚’ã“ã“ã§çµ±ä¸€ã—ã¦è¡Œã†
        content = self._clean_code(content)
        
        path = os.path.abspath(os.path.join(config.OUTPUT_DIR, fname))
        if not path.startswith(os.path.abspath(config.OUTPUT_DIR)): return # ãƒ‘ã‚¹æ¼æ´©é˜²æ­¢
        
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        self.project_files[fname] = content

    def parse_and_save_files(self, llm_response: str, default_filename: str = None) -> Dict[str, str]:
        """LLMã®å‡ºåŠ›ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã¦ä¿å­˜ã™ã‚‹"""
        files = {}
        # # FILENAME: ... ãƒ‘ã‚¿ãƒ¼ãƒ³
        pattern = re.compile(r"^#\s*FILENAME:\s*(?P<name>[^\n]+)\n(?P<code>.*?)(?=^#\s*FILENAME:|\Z)", re.DOTALL | re.MULTILINE)
        matches = list(pattern.finditer(llm_response))
        
        if matches:
            for match in matches:
                fname = match.group("name").strip()
                code = match.group("code").strip()
                files[fname] = code
                self.save_file(fname, code)
        elif default_filename:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒãªã„å ´åˆã¯å…¨ä½“ã‚’ä¸€ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æ‰±ã†
            self.save_file(default_filename, llm_response)
            files[default_filename] = llm_response
            
        return files

    def _clean_code(self, text: str) -> str:
        # ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜æ³•ã®é™¤åŽ»
        return text.replace("```python", "").replace("```json", "").replace("```", "").strip()

    def add_to_requirements(self, pkg: str):
        path = "requirements.txt"
        current = self.project_files.get(path, "")
        if pkg not in current:
            new_content = current.strip() + f"\n{pkg}\n"
            self.save_file(path, new_content)

    def commit(self, message: str):
        self.git.commit(message)


## File: `templates\index.html`

html
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evo Studio v2.6 (Retro IDE)</title>
    
    <!-- CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/vue/3.3.4/vue.global.min.js"></script>
    <!-- ãƒ•ã‚©ãƒ³ãƒˆ: DotGothic16 (æ—¥æœ¬èªž) & Press Start 2P (è‹±æ•°) -->
    <link href="https://fonts.googleapis.com/css2?family=DotGothic16&family=Press+Start+2P&display=swap" rel="stylesheet">
    <!-- FontAwesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css">

    <style>
        /* ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆ (è½ã¡ç€ã„ãŸãƒ¬ãƒˆãƒ­ãƒ€ãƒ¼ã‚¯) */
        :root {
            --bg-main: #282c34;
            --bg-panel: #21252b;
            --bg-header: #181a1f;
            --accent-primary: #98c379; /* ãƒ¬ãƒˆãƒ­ã‚°ãƒªãƒ¼ãƒ³ */
            --accent-secondary: #61afef; /* ãƒ¬ãƒˆãƒ­ãƒ–ãƒ«ãƒ¼ */
            --text-main: #abb2bf;
            --text-highlight: #ffffff;
            --border-color: #3e4451;
        }

        body { 
            /* æ—¥æœ¬èªžã¯ DotGothic16, è‹±æ•°å­—ã¯ Press Start 2P */
            font-family: 'Press Start 2P', 'DotGothic16', sans-serif;
            background-color: var(--bg-main);
            color: var(--text-main);
            font-size: 12px;
            line-height: 1.5;
            overflow: hidden;
        }

        .jp-font {
            font-family: 'DotGothic16', sans-serif;
            font-weight: bold;
        }

        /* UIãƒ‘ãƒ¼ãƒ„ */
        .retro-btn {
            background-color: var(--bg-panel);
            color: var(--accent-secondary);
            border: 1px solid var(--border-color);
            transition: all 0.1s;
            font-family: 'DotGothic16', sans-serif;
            cursor: pointer;
        }
        .retro-btn:hover {
            background-color: var(--border-color);
            color: var(--text-highlight);
        }
        .retro-btn:active {
            transform: translateY(2px);
        }
        .retro-btn.primary {
            background-color: var(--accent-primary);
            color: #1e2227;
            border: none;
        }
        .retro-btn.primary:hover {
            opacity: 0.9;
        }

        .retro-input {
            background-color: #1e2227;
            border: 1px solid var(--border-color);
            color: var(--text-highlight);
            font-family: 'DotGothic16', sans-serif;
            font-size: 14px;
        }
        .retro-input:focus {
            outline: none;
            border-color: var(--accent-secondary);
        }

        /* ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒãƒ¼ */
        ::-webkit-scrollbar { width: 8px; height: 8px; }
        ::-webkit-scrollbar-track { background: var(--bg-main); }
        ::-webkit-scrollbar-thumb { background: var(--border-color); border-radius: 4px; }
        ::-webkit-scrollbar-thumb:hover { background: #5c6370; }

        /* ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ */
        .file-item {
            border-bottom: 1px solid var(--bg-header);
            transition: background 0.1s;
        }
        .file-item:hover { 
            background-color: var(--bg-header); 
            color: var(--text-highlight);
            cursor: pointer; 
        }
        .file-item.active { 
            background-color: #323844; 
            color: var(--accent-secondary); 
            border-left: 3px solid var(--accent-secondary);
        }

        /* ãƒã‚¤ãƒ©ã‚¤ãƒˆèª¿æ•´ */
        pre code.hljs {
            font-family: 'Consolas', 'Monaco', monospace; /* ã‚³ãƒ¼ãƒ‰ã¯è¦‹ã‚„ã™ã•é‡è¦– */
            font-size: 14px;
            line-height: 1.6;
            background: transparent;
        }
    </style>
</head>
<body class="h-screen flex flex-col">

    {% raw %}
    <div id="app" class="h-full flex flex-col">
        
        <!-- ãƒ˜ãƒƒãƒ€ãƒ¼ -->
        <header class="h-12 border-b border-gray-700 flex items-center px-4 justify-between shrink-0 bg-[#181a1f]">
            <div class="flex items-center gap-3">
                <i class="fa-solid fa-terminal text-[#98c379] text-xl"></i>
                <div class="flex flex-col">
                    <span class="font-bold text-[14px] tracking-wide text-white jp-font leading-tight">Evo Studio <span class="text-[10px] text-[#61afef]">v2.6</span></span>
                    <span class="text-[8px] text-gray-500">AI AGENT IDE</span>
                </div>
            </div>
            <div class="flex items-center gap-3">
                <div v-if="activeKit" class="text-[10px] px-2 py-1 bg-[#323844] text-[#98c379] border border-gray-600 rounded flex items-center gap-2 jp-font">
                    <i class="fa-solid fa-microchip"></i> {{ activeKit }}
                </div>
                <a href="/download" class="text-[11px] px-3 py-1 retro-btn rounded no-underline flex items-center gap-2 jp-font">
                    <i class="fa-solid fa-download"></i> ä¿å­˜
                </a>
            </div>
        </header>

        <!-- ãƒ¡ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ -->
        <div class="flex-1 flex overflow-hidden">
            
            <!-- å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« -->
            <div class="w-64 border-r border-gray-700 flex flex-col shrink-0 bg-[#21252b]">
                <div class="p-3 text-[11px] font-bold text-gray-400 border-b border-gray-700 flex justify-between items-center jp-font">
                    <span><i class="fa-regular fa-folder-open mr-1"></i> ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</span>
                    <button @click="refreshFiles" class="hover:text-white transition"><i class="fa-solid fa-sync"></i></button>
                </div>
                <div class="flex-1 overflow-y-auto">
                    <div v-for="file in files" :key="file" 
                         @click="loadFile(file)"
                         class="file-item px-4 py-2 text-[12px] flex items-center gap-2"
                         :class="{ 'active': currentFile === file }">
                        <i class="fa-regular fa-file-code text-gray-500"></i> <span class="truncate">{{ file }}</span>
                    </div>
                </div>
                
                <!-- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ‘ãƒãƒ« -->
                <div class="p-3 border-t border-gray-700 bg-[#1e2227]">
                    <div class="text-[10px] font-bold text-gray-500 mb-2 jp-font">ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹</div>
                    <div class="grid grid-cols-2 gap-2 text-[10px] jp-font">
                        <div class="bg-[#282c34] p-1 rounded border border-gray-700 text-center">
                            <div class="text-[#61afef]">ãƒ•ã‚§ãƒ¼ã‚º</div>
                            <div class="text-white">{{ stats.currentPhase }}/{{ stats.totalPhases }}</div>
                        </div>
                        <div class="bg-[#282c34] p-1 rounded border border-gray-700 text-center">
                            <div class="text-[#98c379]">ä¿®å¾©</div>
                            <div class="text-white">{{ stats.l1 + stats.l2 }}</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- ä¸­å¤®: ã‚¨ãƒ‡ã‚£ã‚¿ã‚¨ãƒªã‚¢ -->
            <div class="flex-1 flex flex-col min-w-0 bg-[#282c34]">
                <!-- ã‚¿ãƒ–ãƒãƒ¼ -->
                <div class="h-10 flex border-b border-gray-700 bg-[#21252b]">
                    <button @click="viewMode='code'" class="px-5 text-[11px] flex items-center gap-2 jp-font border-r border-gray-700 transition"
                         :class="viewMode==='code' ? 'bg-[#282c34] text-[#61afef] border-t-2 border-t-[#61afef]' : 'text-gray-500 hover:text-white hover:bg-[#2c313a]'">
                        <i class="fa-solid fa-code"></i> ã‚³ãƒ¼ãƒ‰
                    </button>
                    <button @click="viewMode='preview'" class="px-5 text-[11px] flex items-center gap-2 jp-font border-r border-gray-700 transition"
                         :class="viewMode==='preview' ? 'bg-[#282c34] text-[#61afef] border-t-2 border-t-[#61afef]' : 'text-gray-500 hover:text-white hover:bg-[#2c313a]'">
                        <i class="fa-solid fa-play"></i> ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                    </button>
                    <div class="flex-1"></div>
                    <span v-if="currentFile" class="px-4 text-[11px] text-gray-500 self-center font-mono">{{ currentFile }}</span>
                </div>

                <!-- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¡¨ç¤º -->
                <div class="flex-1 relative overflow-hidden">
                    <div v-show="viewMode==='code'" class="absolute inset-0 overflow-auto">
                        <pre><code class="language-python h-full" ref="codeBlock">{{ fileContent }}</code></pre>
                    </div>
                    <div v-show="viewMode==='preview'" class="absolute inset-0 bg-white">
                        <iframe v-if="previewUrl" :src="previewUrl" class="w-full h-full border-none"></iframe>
                        <div v-else class="flex flex-col items-center justify-center h-full text-[#333]">
                            <i class="fa-solid fa-eye-slash text-4xl mb-4 text-gray-300"></i>
                            <span class="text-[12px] font-bold text-gray-400 jp-font">ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ãã¾ã›ã‚“</span>
                        </div>
                    </div>
                </div>

                <!-- ä¸‹éƒ¨: ã‚³ãƒ³ã‚½ãƒ¼ãƒ«/ãƒãƒ£ãƒƒãƒˆ -->
                <div class="h-1/3 min-h-[150px] border-t border-gray-700 flex flex-col bg-[#21252b]">
                    <div class="h-8 px-4 flex items-center justify-between bg-[#1b1d23] border-b border-gray-700">
                        <span class="text-[11px] font-bold text-[#98c379] jp-font"><i class="fa-solid fa-terminal mr-2"></i>ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚° / ãƒãƒ£ãƒƒãƒˆ</span>
                        <button @click="reset" class="text-[10px] text-gray-500 hover:text-[#e06c75] jp-font"><i class="fa-solid fa-trash mr-1"></i>ãƒ­ã‚°æ¶ˆåŽ»</button>
                    </div>
                    <div class="flex-1 overflow-y-auto p-4 font-mono text-[12px] space-y-3 bg-[#282c34]" ref="chatLog">
                        <div v-for="(msg, i) in messages" :key="i" class="flex gap-3">
                            <div class="font-bold shrink-0 w-12 text-right" :class="{'text-[#61afef]': msg.role==='user', 'text-[#98c379]': msg.role!=='user'}">
                                {{ msg.role === 'user' ? 'YOU' : 'EVO' }}
                            </div>
                            <div class="text-[#abb2bf] flex-1 jp-font leading-relaxed border-l-2 border-gray-700 pl-3" v-html="formatMessage(msg.content)"></div>
                        </div>
                    </div>
                    
                    <!-- å…¥åŠ›ã‚¨ãƒªã‚¢ -->
                    <div class="p-3 border-t border-gray-700 flex gap-3 bg-[#21252b]">
                        <div class="relative flex-1">
                            <i class="fa-solid fa-chevron-right absolute left-3 top-3 text-gray-500 text-xs"></i>
                            <input v-model="prompt" @keydown.enter="generate" 
                                class="w-full retro-input pl-8 pr-4 py-2.5 rounded shadow-inner"
                                placeholder="Evoã¸ã®æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„... (ä¾‹: MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç´¹ä»‹ãƒ–ãƒ­ã‚°ã‚’ä½œã£ã¦)">
                        </div>
                        <button @click="generate" :disabled="loading" class="retro-btn primary px-6 py-2 rounded font-bold shadow-lg flex items-center gap-2">
                            <i v-if="loading" class="fa-solid fa-spinner fa-spin"></i>
                            <span v-else>å®Ÿè¡Œ</span>
                        </button>
                    </div>
                </div>
            </div>
        </div>
    </div>
    {% endraw %}

    <script>
        window.onload = () => {
            if (typeof Vue === 'undefined' || !Vue.createApp) {
                console.error("CRITICAL: Vue.js library failed to load even after window load. Check network.");
                alert("Vue.jsã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚");
                return; 
            }
            
            const { createApp, ref, nextTick, onMounted } = Vue; 

            createApp({
                setup() {
                    const prompt = ref('');
                    const messages = ref([]);
                    const loading = ref(false);
                    const viewMode = ref('code'); 
                    const files = ref([]);
                    const currentFile = ref('');
                    const fileContent = ref('');
                    const previewUrl = ref('');
                    const chatLog = ref(null);
                    const codeBlock = ref(null);
                    const stats = ref({ currentPhase: 0, totalPhases: 0, l1: 0, l2: 0, l3: 0 });
                    const activeKit = ref('');

                    const scrollToBottom = () => nextTick(() => { if(chatLog.value) chatLog.value.scrollTop = chatLog.value.scrollHeight; });
                    const formatMessage = (content) => content ? content.replace(/\n/g, '<br>') : "";

                    const speak = (text) => {
                        if ('speechSynthesis' in window) {
                            const uttr = new SpeechSynthesisUtterance(text);
                            uttr.lang = 'ja-JP'; 
                            uttr.rate = 1.0;
                            window.speechSynthesis.speak(uttr);
                        }
                    };

                    const refreshFiles = async () => {
                        try {
                            const res = await fetch('/files');
                            const data = await res.json();
                            files.value = data.files || [];
                        } catch (e) { console.error(e); }
                    };

                    const loadFile = async (filename) => {
                        currentFile.value = filename;
                        try {
                            const res = await fetch(`/files/content?filename=${filename}`);
                            const data = await res.json();
                            fileContent.value = data.content || "";
                            viewMode.value = 'code';
                            
                            nextTick(() => {
                                if (codeBlock.value) {
                                    codeBlock.value.removeAttribute('data-highlighted');
                                    hljs.highlightElement(codeBlock.value); 
                                }
                            });

                            if (filename.endsWith('.html')) {
                                previewUrl.value = `/preview/${filename}`;
                            } else if (filename === 'app.py' || filename === 'main.py') {
                                previewUrl.value = `/preview/index.html`; 
                            }
                        } catch (e) { console.error(e); }
                    };

                    const generate = async () => {
                        if (!prompt.value || loading.value) return;
                        const userPrompt = prompt.value;
                        prompt.value = '';
                        messages.value.push({ role: 'user', content: userPrompt });
                        loading.value = true;
                        scrollToBottom();

                        try {
                            const res = await fetch('/generate', {
                                method: 'POST',
                                headers: { 'Content-Type': 'application/json' },
                                body: JSON.stringify({ prompt: userPrompt })
                            });
                            
                            if (!res.ok) {
                                const errorText = await res.text();
                                throw new Error(`ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ (${res.status}): ${errorText.substring(0, 100)}...`);
                            }
                            
                            const data = await res.json();

                            if (data.stats) stats.value = data.stats;
                            if (data.kit_used) activeKit.value = data.kit_used;
                            if (data.logs) data.logs.forEach(log => messages.value.push({ role: 'system', content: log }));
                            
                            messages.value.push({ role: 'ai', content: data.message || "å®Œäº†ã—ã¾ã—ãŸã€‚" });

                            await refreshFiles();
                            if (data.main_file) loadFile(data.main_file);

                            speak("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚");

                        } catch (e) {
                            messages.value.push({ role: 'ai', content: `ðŸ’¥ ã‚¨ãƒ©ãƒ¼: ${e.message}` });
                        } finally {
                            loading.value = false;
                            scrollToBottom();
                        }
                    };

                    const reset = () => location.reload();

                    onMounted(() => {
                        refreshFiles();
                        messages.value.push({ role: 'system', content: 'Evo Studio èµ·å‹•å®Œäº†ã€‚' });
                    });

                    return { 
                        prompt, messages, loading, viewMode, files, currentFile, fileContent, previewUrl,
                        chatLog, codeBlock, stats, activeKit, generate, refreshFiles, loadFile, reset, formatMessage 
                    };
                }
            }).mount('#app');
        };
    </script>
</body>
</html>


